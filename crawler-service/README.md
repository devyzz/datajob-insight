# ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ ì„œë¹„ìŠ¤

## ğŸ“‹ ê°œìš”
ì›í‹°ë“œ, ì¡ì½”ë¦¬ì•„, ì‚¬ëŒì¸ì˜ IT/ê°œë°œ/ë°ì´í„° ê´€ë ¨ ì±„ìš©ê³µê³ ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ MongoDBì— ì €ì¥í•˜ëŠ” í¬ë¡¤ë§ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°
```
crawler-service/
â”œâ”€â”€ Dockerfile                    # Docker ì´ë¯¸ì§€ ë¹Œë“œ ì„¤ì •
â”œâ”€â”€ docker-compose.yml           # Docker ì„œë¹„ìŠ¤ êµ¬ì„±
â”œâ”€â”€ crontab                      # ì£¼ê¸°ì‹¤í–‰ cron ì„¤ì •
â”œâ”€â”€ .env.example                 # í™˜ê²½ë³€ìˆ˜ ì˜ˆì‹œ íŒŒì¼
â”œâ”€â”€ main.py                     # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run_crawlers.sh            # ë³‘ë ¬ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ pyproject.toml             # Poetry ì˜ì¡´ì„± ê´€ë¦¬
â”œâ”€â”€ requirements.txt           # pip ì˜ì¡´ì„± (ë°±ì—…ìš©)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ site_configs.py        # ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§ ì„¤ì •
â”œâ”€â”€ crawler/                   # í¬ë¡¤ëŸ¬ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ base_crawler.py        # ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ crawler_factory.py     # í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬
â”‚   â”œâ”€â”€ wanted_crawler.py      # ì›í‹°ë“œ í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ saramin_crawler.py     # ì‚¬ëŒì¸ í¬ë¡¤ëŸ¬
â”‚   â””â”€â”€ jobkorea_crawler.py    # ì¡ì½”ë¦¬ì•„ í¬ë¡¤ëŸ¬
â”œâ”€â”€ models/
â”‚   â””â”€â”€ data_models.py         # ë°ì´í„° ëª¨ë¸ ì •ì˜
â”œâ”€â”€ utils/                     # ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
â”‚   â”œâ”€â”€ database.py            # DB ì—°ê²° ê´€ë¦¬
â”‚   â”œâ”€â”€ position_normalizer.py # ì§ë¬´ ì •ê·œí™”
â”‚   â””â”€â”€ tech_stack_normalizer.py # ê¸°ìˆ ìŠ¤íƒ ì •ê·œí™”
â””â”€â”€ logs/                      # ë¡œê·¸ íŒŒì¼ ì €ì¥
```

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
- **ë‹¤ì¤‘ í”Œë«í¼ ì§€ì›**: ì›í‹°ë“œ, ì¡ì½”ë¦¬ì•„, ì‚¬ëŒì¸
- **ì •ê·œí™”ëœ ë°ì´í„°**: ì§ë¬´, ê¸°ìˆ ìŠ¤íƒ, ê²½ë ¥ ë“± ì¼ê´€ëœ í˜•íƒœë¡œ ì €ì¥
- **ì¤‘ë³µ ì œê±°**: URL ê¸°ë°˜ ì¤‘ë³µ ë°©ì§€ ë° MongoDB ì¸ë±ìŠ¤ í™œìš©
- **ë³‘ë ¬ ì²˜ë¦¬**: ì‚¬ì´íŠ¸ë³„ ë…ë¦½ì  í¬ë¡¤ë§
- **Docker ì§€ì›**: ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
- **ì£¼ê¸° ì‹¤í–‰**: cronì„ í†µí•œ ìë™í™” ì§€ì›

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. Docker Compose ì‚¬ìš© (ê¶Œì¥)

#### ì €ì¥ì†Œ í´ë¡  ë° ì„¤ì •
```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd crawler-service

# í•„ìˆ˜ íŒŒì¼ë“¤ ìƒì„±
# crontab íŒŒì¼ ìƒì„± (ìœ„ì˜ crontab ì„¤ì • ì°¸ê³ )

# (ì„ íƒ) í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ìƒì„± (.env)
echo "CRAWLER_SERVICE_MONGODB_URI=mongodb://your-mongodb-host:27017/job_crawler" > .env
```

#### ë‹¨ë°œì„± ì‹¤í–‰ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
```bash
# íŠ¹ì • ì‚¬ì´íŠ¸ë§Œ ì‹¤í–‰
docker-compose run --rm crawler python main.py --site wanted

# ì „ì²´ í¬ë¡¤ë§
docker-compose run --rm crawler python main.py --site wanted --full

# ëª¨ë“  ì‚¬ì´íŠ¸ ë³‘ë ¬ ì‹¤í–‰
docker-compose run --rm crawler ./run_crawlers.sh

# ì»¤ìŠ¤í…€ MongoDB URIë¡œ ì‹¤í–‰
CRAWLER_SERVICE_MONGODB_URI="mongodb://custom-host:27017/job_crawler" \
docker-compose run --rm crawler ./run_crawlers.sh
```

#### ì£¼ê¸°ì  ì‹¤í–‰ (ìš´ì˜ìš©)
```bash
# ì£¼ê¸°ì‹¤í–‰ í¬ë¡¤ëŸ¬ ì‹œì‘
docker-compose up -d crawler-cron

# ë¡œê·¸ í™•ì¸
docker-compose logs -f crawler-cron

# ì„œë¹„ìŠ¤ ì¤‘ë‹¨
docker-compose down
```

### 2. ë¡œì»¬ í™˜ê²½ ì„¤ì •

#### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
- Python 3.12+
- Poetry (ì˜ì¡´ì„± ê´€ë¦¬)
- MongoDB ì ‘ê·¼ ê¶Œí•œ

#### í™˜ê²½ ì¤€ë¹„
```bash
# 1. Poetry ì„¤ì¹˜ (ì—†ëŠ” ê²½ìš°)
curl -sSL https://install.python-poetry.org | python3 -

# ë˜ëŠ” pipë¡œ ì„¤ì¹˜
pip install poetry

# 2. ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd crawler-service

# 3. í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ì„¤ì¹˜
poetry install

# 4. Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜
poetry run playwright install chromium

# 5. í™˜ê²½ë³€ìˆ˜ ì„¤ì • íŒŒì¼ ìƒì„±
cp .env.example .env
# .env íŒŒì¼ì—ì„œ MongoDB URI ìˆ˜ì •
```

#### í™˜ê²½ë³€ìˆ˜ ì„¤ì •
```bash
# ë°©ë²• 1: .env íŒŒì¼ ì‚¬ìš© (ê¶Œì¥)
cp .env.example .env

# .env íŒŒì¼ ë‚´ìš© ìˆ˜ì •
CRAWLER_SERVICE_MONGODB_URI=mongodb://your-mongodb-host:27017/job_crawler
LOG_LEVEL=INFO

# ë°©ë²• 2: ì§ì ‘ export
export CRAWLER_SERVICE_MONGODB_URI="mongodb://your-mongodb-host:27017/job_crawler"
export LOG_LEVEL="INFO"

# ì¸ì¦ì´ í•„ìš”í•œ ê²½ìš°
export CRAWLER_SERVICE_MONGODB_URI="mongodb://username:password@host:27017/job_crawler?authSource=admin"
```

#### Poetry ì‚¬ìš©ë²•
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
poetry install

# ê°œë°œ ì˜ì¡´ì„± í¬í•¨ ì„¤ì¹˜
poetry install --with dev

# ê°€ìƒí™˜ê²½ ì •ë³´ í™•ì¸
poetry env info

# ê°€ìƒí™˜ê²½ í™œì„±í™”
poetry shell

# ê°€ìƒí™˜ê²½ì—ì„œ ì‹¤í–‰ (shell ì§„ì…í•˜ì§€ ì•Šê³ )
poetry run python main.py --site wanted

# ì˜ì¡´ì„± ì¶”ê°€
poetry add requests

# ê°œë°œ ì˜ì¡´ì„± ì¶”ê°€
poetry add --group dev pytest
```

#### í¬ë¡¤ë§ ì‹¤í–‰
```bash
# Poetry ê°€ìƒí™˜ê²½ì—ì„œ ì‹¤í–‰

# ë‹¨ì¼ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
poetry run python main.py --site wanted

# ì „ì²´ í¬ë¡¤ë§
poetry run python main.py --site wanted --full

# MongoDB URI ì§ì ‘ ì§€ì •
poetry run python main.py --site wanted --mongodb-uri mongodb://your-host:27017/job_crawler

# ëª¨ë“  ì‚¬ì´íŠ¸ ë³‘ë ¬ ì‹¤í–‰
poetry run ./run_crawlers.sh

# ì»¤ìŠ¤í…€ MongoDB URIë¡œ ë³‘ë ¬ ì‹¤í–‰
./run_crawlers.sh normal mongodb://your-host:27017/job_crawler

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
poetry run python main.py --site wanted --log-level DEBUG
```

#### ê°€ìƒí™˜ê²½ í™œì„±í™” í›„ ì‹¤í–‰ (ì„ íƒì‚¬í•­)
```bash
# Poetry shell ì§„ì…
poetry shell

# ì´í›„ poetry run ì—†ì´ ì§ì ‘ ì‹¤í–‰ ê°€ëŠ¥
python main.py --site wanted
./run_crawlers.sh

# shell ì¢…ë£Œ
exit
```

#### ê°œë°œ í™˜ê²½ ì„¤ì •
```bash
# ì½”ë“œ í¬ë§·íŒ… (Black)
poetry run black .

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
poetry run pytest

# í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€
poetry run pytest --cov=crawler

# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
poetry update

# requirements.txt ìƒì„± (í•„ìš”ì‹œ)
poetry export -f requirements.txt --output requirements.txt
```

## ğŸ“– ìƒì„¸ ì‚¬ìš©ë²•

### ëª…ë ¹ì–´ ì˜µì…˜
```bash
# ê¸°ë³¸ ì‚¬ìš©ë²•
python main.py --site <ì‚¬ì´íŠ¸ëª…> [ì˜µì…˜]

# ì˜µì…˜ ì„¤ëª…
--site          : í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ (wanted, saramin, jobkorea)
--full          : ì „ì²´ í¬ë¡¤ë§ (ê¸°ë³¸ê°’: ì‹ ê·œë§Œ)
--mongodb-uri   : MongoDB ì—°ê²° URI
--log-level     : ë¡œê·¸ ë ˆë²¨ (DEBUG, INFO, WARNING, ERROR)

# ì˜ˆì‹œ
python main.py --site wanted --full --log-level DEBUG
```

### ë³‘ë ¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```bash
# run_crawlers.sh ì‚¬ìš©ë²•
./run_crawlers.sh [mode] [mongodb_uri]

# mode: normal(ê¸°ë³¸ê°’) | full
# mongodb_uri: MongoDB ì—°ê²° ì£¼ì†Œ

# ì˜ˆì‹œ
./run_crawlers.sh                                    # ê¸°ë³¸ ì‹¤í–‰
./run_crawlers.sh full                              # ì „ì²´ í¬ë¡¤ë§
./run_crawlers.sh normal mongodb://remote:27017/   # ì›ê²© DB
```

## â° ì£¼ê¸°ì  ì‹¤í–‰ ì„¤ì •

### Docker í™˜ê²½ì—ì„œ cron ì„¤ì •
```bash
# crontab íŒŒì¼ ë‚´ìš© í™•ì¸
cat crontab

# Docker Composeë¡œ ì£¼ê¸°ì‹¤í–‰ ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d crawler-cron

# cron ë¡œê·¸ í™•ì¸
docker-compose exec crawler-cron tail -f /app/logs/cron.log
```

### ë¡œì»¬ í™˜ê²½ì—ì„œ cron ì„¤ì •
```bash
# crontab í¸ì§‘
crontab -e

# ë‹¤ìŒ ë‚´ìš© ì¶”ê°€
0 9 * * * cd /path/to/crawler-service && ./run_crawlers.sh >> logs/cron.log 2>&1
0 18 * * * cd /path/to/crawler-service && ./run_crawlers.sh >> logs/cron.log 2>&1

# cron ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
sudo systemctl status cron
```

## ğŸ—‚ï¸ ë°ì´í„° êµ¬ì¡°

ìˆ˜ì§‘ëœ ì±„ìš©ê³µê³ ëŠ” ë‹¤ìŒ êµ¬ì¡°ë¡œ ì €ì¥ë©ë‹ˆë‹¤:

```json
{
  "_id": "ObjectId",
  "job_id": "platform_year_identifier",
  "job_url": "ì›ë³¸ URL",
  "platform": "wanted|saramin|jobkorea",
  "job_title": "ì±„ìš©ê³µê³  ì œëª©",
  "company": {
    "name": "íšŒì‚¬ëª…",
    "size": "íšŒì‚¬ ê·œëª¨",
    "industry": "ì—…ì¢…"
  },
  "location": {
    "raw_text": "ì›ë³¸ ì§€ì—­ ì •ë³´",
    "city": "ì‹œ/ë„",
    "district": "êµ¬/êµ°"
  },
  "experience": {
    "raw_text": "ì›ë³¸ ê²½ë ¥ ì •ë³´",
    "min_years": 0,
    "max_years": 10
  },
  "position": {
    "raw_text": "ì›ë³¸ ì§ë¬´",
    "normalized": {
      "primary_category": "ë¶„ë¥˜",
      "secondary_category": "ì„¸ë¶€ë¶„ë¥˜",
      "is_data_role": true
    }
  },
  "tech_stack": {
    "raw_text": "ì›ë³¸ ê¸°ìˆ ìŠ¤íƒ",
    "raw_list": ["Python", "SQL"]
  },
  "crawled_at": "í¬ë¡¤ë§ ì‹œê°„"
}
```

## âš™ï¸ í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ëª… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|--------|--------|------|
| `CRAWLER_SERVICE_MONGODB_URI` | `mongodb://localhost:27017/` | MongoDB ì—°ê²° URI |
| `LOG_LEVEL` | `INFO` | ë¡œê·¸ ë ˆë²¨ |
| `TZ` | `Asia/Seoul` | ì‹œê°„ëŒ€ ì„¤ì • |

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬

### ë¡œê·¸ í™•ì¸
```bash
# Docker í™˜ê²½
docker-compose logs -f crawler-cron
docker-compose logs crawler

# ë¡œì»¬ í™˜ê²½
tail -f logs/*/crawler_*.log
tail -f logs/cron.log

# íŠ¹ì • ì‚¬ì´íŠ¸ ë¡œê·¸ë§Œ í™•ì¸
tail -f logs/*/crawler_wanted.log
```

### ë°ì´í„° í™•ì¸
ì™¸ë¶€ MongoDB ê´€ë¦¬ ë„êµ¬ë‚˜ ì§ì ‘ ì—°ê²°í•˜ì—¬ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”:
```bash
# ì˜ˆì‹œ: MongoDB Compass ë˜ëŠ” mongo shell ì‚¬ìš©
# ì»¬ë ‰ì…˜: wanted_job_postings, saramin_job_postings, jobkorea_job_postings
```

### í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬
```bash
# ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ëŸ¬ í™•ì¸
ps aux | grep 'python main.py'

# ëª¨ë“  í¬ë¡¤ëŸ¬ ê°•ì œ ì¢…ë£Œ
pkill -f 'python main.py'

# Docker ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose restart crawler-cron

# Docker ì´ë¯¸ì§€ ì¬ë¹Œë“œ (ì½”ë“œ ë³€ê²½ì‹œ)
docker-compose build --no-cache
```

## ğŸ”§ ê°œë°œ ì •ë³´

### í¬ë¡¤ë§ ì½”ë“œ êµ¬ì¡°
[[]]

### ê¸°ìˆ  ìŠ¤íƒ
- **ì–¸ì–´**: Python 3.12
- **ì˜ì¡´ì„± ê´€ë¦¬**: Poetry
- **ì›¹ í¬ë¡¤ë§**: Requests, BeautifulSoup, Playwright
- **ë°ì´í„°ë² ì´ìŠ¤**: MongoDB, PyMongo
- **ì»¨í…Œì´ë„ˆí™”**: Docker, Docker Compose

### í™•ì¥ ë°©ë²•
ìƒˆë¡œìš´ í”Œë«í¼ ì¶”ê°€ì‹œ:
1. `crawler/` ë””ë ‰í† ë¦¬ì— ìƒˆ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ìƒì„±
2. `JobCrawler` ê¸°ë³¸ í´ë˜ìŠ¤ ìƒì†
3. `_get_job_urls()`, `_crawl_job_detail()` ë©”ì„œë“œ êµ¬í˜„
4. `config/site_configs.py`ì— ì„¤ì • ì¶”ê°€
5. `CrawlerFactory`ì— ë“±ë¡

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° íŒ

### ì‹¤í–‰ ê´€ë ¨
- **MongoDB ì—°ê²°**: í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì „ MongoDB ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
- **ë°©í™”ë²½**: MongoDB í¬íŠ¸(ê¸°ë³¸ 27017)ê°€ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸
- **ë””ìŠ¤í¬ ê³µê°„**: ë¡œê·¸ íŒŒì¼ì´ ê³„ì† ìŒ“ì´ë¯€ë¡œ ì£¼ê¸°ì  ì •ë¦¬ í•„ìš”
```bash
# 7ì¼ ì´ì „ ë¡œê·¸ íŒŒì¼ ì •ë¦¬
find logs/ -name "*.log" -mtime +7 -delete
```

### ë´‡ ì°¨ë‹¨ ë°©ì§€
- **ë”œë ˆì´ ì¤€ìˆ˜**: ê° ì‚¬ì´íŠ¸ë³„ ì„¤ì •ëœ ë”œë ˆì´ ì‹œê°„ ì¤€ìˆ˜
- **IP ë¶„ì‚°**: ê°€ëŠ¥í•˜ë©´ ì—¬ëŸ¬ IPì—ì„œ ë¶„ì‚° ì‹¤í–‰
- **ì‹œê°„ì°¨ ì‹¤í–‰**: 3ê°œ ì‚¬ì´íŠ¸ ë™ì‹œ ì‹¤í–‰ë³´ë‹¤ëŠ” ì‹œê°„ì°¨ë¥¼ ë‘ê³  ì‹¤í–‰
```bash
# ì‹œê°„ì°¨ë¥¼ ë‘” ì‹¤í–‰ ì˜ˆì‹œ
python main.py --site wanted && sleep 300 && \
python main.py --site saramin && sleep 300 && \
python main.py --site jobkorea
```

### ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
- **ë©”ëª¨ë¦¬**: PlaywrightëŠ” ë¸Œë¼ìš°ì €ë‹¹ 100-200MB ì‚¬ìš©
- **CPU**: ë³‘ë ¬ ì‹¤í–‰ì‹œ CPU ì‚¬ìš©ëŸ‰ ê¸‰ì¦ ê°€ëŠ¥
- **ë„¤íŠ¸ì›Œí¬**: ëŒ€ëŸ‰ í¬ë¡¤ë§ì‹œ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ê³ ë ¤

### ê°œë°œ/ìš´ì˜ ë¶„ë¦¬
```bash
# ê°œë°œí™˜ê²½ (ì ì€ í˜ì´ì§€)
export CRAWL_PAGES=3
python main.py --site wanted

# ìš´ì˜í™˜ê²½ (ì „ì²´ í˜ì´ì§€)
python main.py --site wanted --full
```

### ì—ëŸ¬ ëŒ€ì‘
```bash
# íŠ¹ì • ì‚¬ì´íŠ¸ê°€ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°
python main.py --site wanted --log-level DEBUG

# Playwright ë¸Œë¼ìš°ì € ì¬ì„¤ì¹˜
poetry run playwright install --force chromium

# Docker ì´ë¯¸ì§€ ì™„ì „ ì¬ë¹Œë“œ
docker-compose build --no-cache

# ì»¨í…Œì´ë„ˆ ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
docker-compose logs -f crawler-cron
```

## ğŸ“ ë¼ì´ì„ ìŠ¤
MIT License