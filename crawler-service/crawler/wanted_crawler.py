import time
import re
from bs4 import BeautifulSoup
from datetime import datetime
from time import sleep
from playwright.sync_api import sync_playwright

from crawler.base_crawler import JobCrawler
from models.data_models import JobPostingModel
from utils.position_normalizer import normalize_position_data
from config.site_configs import SITE_CONFIGS, COMMON_BROWSER_HEADERS

class WantedCrawler(JobCrawler):
    """ì›í‹°ë“œ ì „ìš© í¬ë¡¤ëŸ¬"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._playwright = None
        self._browser = None
        self._context = None
    
    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ìž…"""
        super().__enter__()
        self._init_playwright()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self._cleanup_playwright()
        super().__exit__(exc_type, exc_val, exc_tb)
    
    def _init_playwright(self):
        """Playwright ì´ˆê¸°í™” (lazy loading)"""
        try:
            if not self._playwright:
                self._playwright = sync_playwright().start()
                self._browser = self._playwright.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-dev-shm-usage']
                )
                self._context = self._browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                self.logger.info("âœ… Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ Playwright ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._playwright = None
    
    def _cleanup_playwright(self):
        """Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self._context:
                self._context.close()
            if self._browser:
                self._browser.close()
            if self._playwright:
                self._playwright.stop()
            self.logger.info("âœ… Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"âŒ Playwright ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _get_job_urls(self, config, full_crawl: bool = False) -> list:
        """
        ì›í‹°ë“œ APIë¡œ URL ìˆ˜ì§‘
        """
        job_urls = []
        
        # ì„¸ì…˜ í—¤ë” ì—…ë°ì´íŠ¸
        self.session.headers.update(config.requests_options.get('headers', {}))
        
        # ì „ì²´ í¬ë¡¤ë§ì´ë©´ ì¶©ë¶„ížˆ í° íŽ˜ì´ì§€ ìˆ˜, ì•„ë‹ˆë©´ ì„¤ì •ê°’ ì‚¬ìš©
        max_pages = 100000 if full_crawl else config.max_pages
        consecutive_empty = 0  # ì—°ì†ìœ¼ë¡œ ë¹ˆ ì‘ë‹µ ì¹´ìš´íŠ¸
        
        # ê°œë°œì „ì²´ - ì±„ìš©ê³µê³  ì¡°íšŒ API
        successful_api = {
                'name' : 'v1_chaos', 
                'url' : 'https://www.wanted.co.kr/api/chaos/navigation/v1/results',
                'params_template' : {
                    'country' : 'kr',
                    'job_sort' : 'job.latest_order',
                    'years' : -1,
                    'locations' : 'all',
                    'limit' : 20, 
                    'job_group_id' : 518
                }
            }
        
        for page in range(0, max_pages):
            try:
                params = {**successful_api['params_template'], 'offset': page * 20}
                response = self.session.get(successful_api['url'], params=params, timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    jobs = data.get('data', [])
                else:
                    self.logger.warning(f"âŒ {successful_api['name']} ì‹¤íŒ¨: {response.status_code}")
                    continue
                
                self.logger.info(f"ðŸ”— ì›í‹°ë“œ API í˜¸ì¶œ ì¤‘ (page {page + 1}, API: {successful_api['name'] if successful_api else 'Unknown'})...")
                
                if not jobs:
                    consecutive_empty += 1
                    self.logger.info(f"ðŸ“„ ë¹ˆ ì‘ë‹µ ({consecutive_empty}/3)")
                    
                    # ì—°ì† 3ë²ˆ ë¹ˆ ì‘ë‹µì´ë©´ ì¢…ë£Œ
                    if consecutive_empty >= 3:
                        self.logger.info("ðŸ›‘ ë” ì´ìƒ ì±„ìš©ê³µê³ ê°€ ì—†ì–´ì„œ ì¢…ë£Œ")
                        break
                    continue
                else:
                    consecutive_empty = 0  # ë°ì´í„°ê°€ ìžˆìœ¼ë©´ ë¦¬ì…‹
                
                for job in jobs:
                    job_url = f"https://www.wanted.co.kr/wd/{job.get('id')}"
                    if job_url:
                        job_urls.append(job_url)
                        
                        self.logger.info(f"âœ… [FOUND] ì±„ìš©ê³µê³  | íšŒì‚¬: {job.get('company', {}).get('name', 'Unknown')} | í¬ì§€ì…˜: {job.get('position') or job.get('title', 'Unknown')} | URL: {job_url}")
                
                time.sleep(config.delay)
                
            except Exception as e:
                self.logger.error(f"âŒ ì›í‹°ë“œ page {page} ì˜¤ë¥˜: {e}")
                
        return list(set(job_urls))
    
    def _crawl_job_detail(self, url: str, config) -> JobPostingModel:
        """
        ì›í‹°ë“œ ìƒì„¸ íŽ˜ì´ì§€ í¬ë¡¤ë§ - API + HTML íŒŒì‹± ë°©ì‹
        """
        
        # URLì—ì„œ job_id ì¶”ì¶œ
        job_id_match = re.search(r'/wd/(\d+)', url)
        if not job_id_match:
            self.logger.error(f"âŒ ì›í‹°ë“œ URLì—ì„œ job_id ì¶”ì¶œ ì‹¤íŒ¨: {url}")
            return None
        
        job_id = job_id_match.group(1)
        self.logger.info(f"ðŸ” ì›í‹°ë“œ ìƒì„¸ í¬ë¡¤ë§ ì‹œìž‘: url={url} | job_id={job_id}")
        
        try:
            # APIë¡œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            detail_data = self._fetch_wanted_detail_api(job_id)
            if not detail_data:
                self.logger.error(f"âŒ ì›í‹°ë“œ ìƒì„¸ API í˜¸ì¶œ ì‹¤íŒ¨: job_id={job_id}")
                return None
            
            # íšŒì‚¬ íŽ˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë§¤ì¶œ ì •ë³´ í¬í•¨)
            company_info = self._fetch_wanted_company_info(detail_data)
            
            # ëª¨ë“  ì •ë³´ë¥¼ detail_dataì— í†µí•©
            detail_data = self._append_company_info_to_detail_data(detail_data, company_info)
            
            # JobPostingModelë¡œ íŒŒì‹±
            job_posting = self._parse_wanted_detail_to_jobposting(detail_data, url)
            sleep(config.delay)
            
            return job_posting
            
        except Exception as e:
            self.logger.error(f"âŒ ì›í‹°ë“œ í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            return None

    def _fetch_wanted_detail_api(self, job_id: str) -> dict:
        """
        ì›í‹°ë“œ ìƒì„¸ ì •ë³´ API í˜¸ì¶œ
        """
        try:
            # API URL êµ¬ì„±
            timestamp = int(time.time() * 1000)
            detail_url = f"https://www.wanted.co.kr/api/chaos/jobs/v4/{job_id}/details"
            
            # configì—ì„œ í—¤ë” ê°€ì ¸ì˜¤ê¸°
            config = self.session.headers.copy()
            headers = {
                'Accept': 'application/json, text/plain, */*',
                'Referer': f'https://www.wanted.co.kr/wd/{job_id}',
                'sec-ch-ua': '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"Windows"',
                'X-Requested-With': 'XMLHttpRequest'
            }
            headers.update(config)
            
            # API í˜¸ì¶œ ì‹¤í–‰
            response = self.session.get(
                detail_url, 
                headers=headers,
                params={str(timestamp): ''},
                timeout=30
            )
            
            # ì‘ë‹µ ìƒíƒœ ê²€ì¦
            if response.status_code == 200:
                data = response.json()
                
                if data.get('message') == 'ok' and data.get('data'):
                    return data['data']
                else:
                    self.logger.warning(f"âš ï¸ ì›í‹°ë“œ API ì‘ë‹µ êµ¬ì¡° ì˜¤ë¥˜: {data.get('message', 'Unknown')}")
                    return None
            else:
                self.logger.warning(f"âš ï¸ ì›í‹°ë“œ API HTTP ì˜¤ë¥˜: {response.status_code} for job_id={job_id}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ì›í‹°ë“œ ìƒì„¸ API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return None

    def _fetch_wanted_company_info(self, detail_data: dict) -> dict:
        """
        ì›í‹°ë“œ íšŒì‚¬ ì •ë³´ íŽ˜ì´ì§€ í¬ë¡¤ë§ - BeautifulSoup ì‚¬ìš©
        """
        
        try:
            # íšŒì‚¬ ID ì¶”ì¶œ
            job_data = detail_data.get('job', {})
            company_data = job_data.get('company', {})
            company_id = company_data.get('id')
            
            if not company_id:
                self.logger.info("âš ï¸ íšŒì‚¬ IDê°€ ì—†ì–´ì„œ íšŒì‚¬ ì •ë³´ íŽ˜ì´ì§€ í¬ë¡¤ë§ ìŠ¤í‚µ")
                return {}
            
            # ì›í‹°ë“œ íšŒì‚¬ ì •ë³´ íŽ˜ì´ì§€ URL êµ¬ì„±
            company_url = f"https://www.wanted.co.kr/company/{company_id}"
            
            self.logger.debug(f"ðŸ” íšŒì‚¬ ì •ë³´ í¬ë¡¤ë§: {company_data.get('name')} | {company_url}")
            
            # configì—ì„œ í—¤ë” ê°€ì ¸ì˜¤ê¸°
            
            headers = COMMON_BROWSER_HEADERS
            
            # íšŒì‚¬ íŽ˜ì´ì§€ ìš”ì²­
            response = self.session.get(company_url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                self.logger.info(f"âš ï¸ íšŒì‚¬ ì •ë³´ íŽ˜ì´ì§€ HTTP ì˜¤ë¥˜: {response.status_code}")
                return {}
            
            # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # CompanyInfoTableì—ì„œ ì •ë³´ ì¶”ì¶œ
            company_info = self._parse_company_info_table(soup, company_url)
            
            if company_info:
                self.logger.debug(f"âœ… íšŒì‚¬ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {len(company_info)}ê°œ í•­ëª©")
                return company_info
            else:
                self.logger.info("âš ï¸ íšŒì‚¬ ì •ë³´ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return {}
                
        except Exception as e:
            self.logger.info(f"íšŒì‚¬ ì •ë³´ íŽ˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ (ì„ íƒì  ê¸°ëŠ¥): {e}")
            return {}

    def _parse_company_info_table(self, soup, company_url: str) -> dict:
        """íšŒì‚¬ ì •ë³´ í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ - ë§¤ì¶œ ì •ë³´ í¬í•¨"""

        company_info = {}

        try:
            # 1. ê¸°ì¡´ CompanyInfoTableì—ì„œ í‘œì¤€ì‚°ì—…ë¶„ë¥˜ ì¶”ì¶œ
            company_info_div = soup.select_one('div[class^="CompanyInfoTable_wrapper"]')
            if company_info_div:
                self.logger.debug("âœ… CompanyInfoTable_wrapper ì„¹ì…˜ ë°œê²¬")
                
                definition_lists = company_info_div.find_all('dl', class_=lambda x: x and 'CompanyInfoTable_definition' in x)

                for dl in definition_lists:
                    try:
                        dt_element = dl.find('dt', class_=lambda x: x and 'CompanyInfoTable_definition__dt' in x)
                        dd_element = dl.find('dd', class_=lambda x: x and 'CompanyInfoTable_definition__dd' in x)

                        if not dt_element or not dd_element:
                            continue

                        key = dt_element.get_text(strip=True)
                        
                        # í‘œì¤€ì‚°ì—…ë¶„ë¥˜
                        if key == 'í‘œì¤€ì‚°ì—…ë¶„ë¥˜':
                            value = dd_element.get_text(strip=True)
                            if value and value != '-':
                                company_info['industry_classification'] = value
                                self.logger.debug(f"âœ… í‘œì¤€ì‚°ì—…ë¶„ë¥˜: {value}")

                        # ê¸°ì¡´ ë§¤ì¶œì•¡ (ì°¸ê³ ìš©)
                        elif key == 'ë§¤ì¶œì•¡':
                            value = self._extract_revenue_value(dd_element)
                            if value and value != '-':
                                company_info['old_revenue'] = value
                                self.logger.debug(f"â„¹ï¸ ê¸°ì¡´ ë§¤ì¶œì•¡: {value}")

                    except Exception as e:
                        self.logger.debug(f"âŒ í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        continue

            # 2. ë§¤ì¶œ ì°¨íŠ¸ì—ì„œ ìµœì‹  ë§¤ì¶œ ì •ë³´ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ ë†’ìŒ) - Playwright ì‚¬ìš©
            sales_info = self._extract_sales_from_chart(company_url)
            if sales_info:
                company_info['revenue'] = sales_info
                self.logger.debug(f"âœ… ìµœì‹  ë§¤ì¶œ ì •ë³´ ì¶”ì¶œ: {sales_info}")
            elif company_info.get('old_revenue'):
                # ì°¨íŠ¸ì—ì„œ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ê¸°ì¡´ ë§¤ì¶œì•¡ ì‚¬ìš©
                company_info['revenue'] = company_info['old_revenue']
                self.logger.debug(f"ðŸ“‹ ê¸°ì¡´ ë§¤ì¶œì•¡ ì‚¬ìš©: {company_info['old_revenue']}")

            return company_info

        except Exception as e:
            self.logger.error(f"âŒ íšŒì‚¬ ì •ë³´ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {}
        
    def _extract_sales_from_chart(self, company_url: str) -> str:
        """
        Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  ì°¨íŠ¸ì—ì„œ ë§¤ì¶œ ì •ë³´ ì¶”ì¶œ
        """
        try:
            # Playwrightê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ˆê¸°í™”
            if not self._playwright:
                self._init_playwright()
            
            if not self._context:
                self.logger.error("âŒ Playwright ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
                return ""
            
            # ìƒˆ íŽ˜ì´ì§€ ìƒì„±
            page = self._context.new_page()
            
            try:
                self.logger.debug(f"ðŸŒ Playwrightë¡œ íŽ˜ì´ì§€ ë¡œë“œ ì¤‘: {company_url}")
                
                # íŽ˜ì´ì§€ ë¡œë“œ
                page.goto(company_url, wait_until='domcontentloaded', timeout=30000)
                
                # ì°¨íŠ¸ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ìž ì‹œ ëŒ€ê¸°
                page.wait_for_timeout(2000)
                
                # ë°©ë²• 1: ChartSummary + wds í´ëž˜ìŠ¤ ë°©ì‹
                self.logger.debug("ðŸ” ë°©ë²• 1: SalesChart + wds í´ëž˜ìŠ¤ ë°©ì‹")
                chart_wrapper = page.query_selector('div[class*="SalesChart_wrapper"]')
                if chart_wrapper:
                    self.logger.debug("âœ… SalesChart_wrapper ë°œê²¬")
                    
                    # wds í´ëž˜ìŠ¤ë¥¼ ê°€ì§„ ëª¨ë“  ìš”ì†Œ ì°¾ê¸°
                    wds_elements = chart_wrapper.query_selector_all('div[class*="wds"]')
                    self.logger.debug(f"ðŸ“‹ wds ìš”ì†Œ ìˆ˜: {len(wds_elements)}")
                    
                    for i, element in enumerate(wds_elements):
                        text = element.text_content().strip()
                        self.logger.debug(f"  wds ìš”ì†Œ {i+1}: '{text}'")
                        
                        # ìˆ«ìžì™€ "ì›"ì´ í¬í•¨ëœ ê²ƒ ì°¾ê¸° (ë§¤ì¶œ ë¼ë²¨ ì œì™¸)
                        if (any(char.isdigit() for char in text) and 
                            any(unit in text for unit in ['ë§Œì›', 'ì–µì›', 'ì²œì›', 'ì›']) and
                            'ë§¤ì¶œ' not in text):
                            self.logger.debug(f"âœ… ë°©ë²•1 ì„±ê³µ! ì°¨íŠ¸ì—ì„œ ë§¤ì¶œê°’ ì¶”ì¶œ: {text}")
                            return text
                    
                    self.logger.debug("âŒ ë°©ë²•1 ì‹¤íŒ¨: ì í•©í•œ wds ìš”ì†Œ ì—†ìŒ")
                else:
                    self.logger.debug("âŒ ë°©ë²•1 ì‹¤íŒ¨: ChartSummary_wrapper ì—†ìŒ")
                
                # ë°©ë²• 2: ë§¤ì¶œ ë¼ë²¨ ê¸°ë°˜ ê²€ìƒ‰
                self.logger.debug("ðŸ” ë°©ë²• 2: ë§¤ì¶œ ë¼ë²¨ ê¸°ë°˜ ê²€ìƒ‰")
                
                # ë§¤ì¶œì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ìš”ì†Œ ì°¾ê¸°
                sales_elements = page.query_selector_all('text=ë§¤ì¶œ')
                sales_elements.extend(page.query_selector_all('[text*="ë§¤ì¶œ"]'))
                
                for sales_element in sales_elements:
                    try:
                        self.logger.debug(f"âœ… ë§¤ì¶œ ë¼ë²¨ ë°œê²¬: '{sales_element.text_content().strip()}'")
                        
                        # ë¶€ëª¨ ì»¨í…Œì´ë„ˆì—ì„œ í˜•ì œ ìš”ì†Œë“¤ ì°¾ê¸°
                        parent = sales_element.locator('..')
                        if parent:
                            # í˜•ì œ ìš”ì†Œë“¤ì—ì„œ ìˆ«ìž+ì› íŒ¨í„´ ì°¾ê¸°
                            siblings = parent.query_selector_all('div, span')
                            
                            for sibling in siblings:
                                text = sibling.text_content().strip()
                                if text and text != sales_element.text_content().strip():
                                    self.logger.debug(f"  í˜•ì œ ìš”ì†Œ: '{text}'")
                                    
                                    if (any(char.isdigit() for char in text) and 
                                        any(unit in text for unit in ['ë§Œì›', 'ì–µì›', 'ì²œì›', 'ì›'])):
                                        self.logger.info(f"âœ… ë°©ë²•2 ì„±ê³µ! ë¼ë²¨ ê¸°ë°˜ ë§¤ì¶œê°’ ì¶”ì¶œ: {text}")
                                        return text
                    except Exception as e:
                        self.logger.debug(f"âŒ ë§¤ì¶œ ë¼ë²¨ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                self.logger.debug("âŒ ëª¨ë“  ë°©ë²• ì‹¤íŒ¨: ì°¨íŠ¸ì—ì„œ ë§¤ì¶œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return ""
                
            finally:
                page.close()
                
        except Exception as e:
            self.logger.error(f"âŒ Playwright ë§¤ì¶œ ì°¨íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return ""

    def _extract_revenue_value(self, dd_element) -> str:
        """ë§¤ì¶œì•¡ ê°’ ì¶”ì¶œ - trailing text ì²˜ë¦¬"""
        try:
            # trailing text ìš”ì†Œë“¤ ì°¾ê¸°
            trailing_texts = dd_element.find_all('div', class_=lambda x: x and 'trailingText' in x)
            
            if trailing_texts:
                # trailing text ì œê±°í•œ ë©”ì¸ í…ìŠ¤íŠ¸
                dd_copy = dd_element.__copy__()
                for trailing in dd_copy.find_all('div', class_=lambda x: x and 'trailingText' in x):
                    trailing.decompose()
                main_text = dd_copy.get_text(strip=True)
                
                # trailing text ìˆ˜ì§‘
                trailing_values = [t.get_text(strip=True) for t in trailing_texts]
                
                # ê²°í•© (ì˜ˆ: "25ì–µ 3,353ë§Œ" + "ì›" = "25ì–µ 3,353ë§Œì›")
                if main_text and trailing_values:
                    return main_text + ''.join(trailing_values)
            
            # ê¸°ë³¸ í…ìŠ¤íŠ¸
            return dd_element.get_text(strip=True)
            
        except Exception:
            return dd_element.get_text(strip=True) if dd_element else ""

    def _append_company_info_to_detail_data(self, detail_data: dict, company_info: dict) -> dict:
        """íšŒì‚¬ ì •ë³´ë¥¼ detail_dataì— í†µí•©"""
        
        if not company_info:
            return detail_data
        
        try:
            # êµ¬ì¡° ë³´ìž¥
            if 'job' not in detail_data:
                detail_data['job'] = {}
            if 'company' not in detail_data['job']:
                detail_data['job']['company'] = {}
            
            company_section = detail_data['job']['company']
            
            # í‘œì¤€ì‚°ì—…ë¶„ë¥˜ ì¶”ê°€ (ê¸°ì¡´ industry_nameì´ ì—†ëŠ” ê²½ìš°ë§Œ)
            if company_info.get('industry_classification'):
                if not company_section.get('industry_name'):
                    company_section['industry_name'] = company_info['industry_classification']
                    self.logger.debug(f"âœ… í‘œì¤€ì‚°ì—…ë¶„ë¥˜ ì¶”ê°€: {company_info['industry_classification']}")
            
            # ë§¤ì¶œì•¡ ì •ë³´ ì¶”ê°€ (íšŒì‚¬ íŽ˜ì´ì§€ì˜ ìµœì‹  ì •ë³´ ìš°ì„ )
            if company_info.get('revenue'):
                company_section['revenue'] = company_info['revenue']
                company_section['sales'] = company_info['revenue']  # í˜¸í™˜ì„±ì„ ìœ„í•´ ë‘˜ ë‹¤ ì„¤ì •
                self.logger.debug(f"âœ… íšŒì‚¬ íŽ˜ì´ì§€ì—ì„œ ë§¤ì¶œì•¡ ì •ë³´ ì¶”ê°€: {company_info['revenue']}")
            
            return detail_data
            
        except Exception as e:
            self.logger.error(f"âŒ íšŒì‚¬ ì •ë³´ í†µí•© ì˜¤ë¥˜: {e}")
            return detail_data

    def _parse_wanted_detail_to_jobposting(self, detail_data: dict, url: str) -> JobPostingModel:
        """
        ì›í‹°ë“œ API ë°ì´í„°ë¥¼ JobPostingModelë¡œ ë³€í™˜
        """
        
        # API ì‘ë‹µ êµ¬ì¡°ì—ì„œ í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ
        job_data = detail_data.get('job', {})
        job_detail = job_data.get('detail', {})
        company_data = job_data.get('company', {})
        address_data = job_data.get('address', {})
        
        # ================== ê¸°ë³¸ ì‹ë³„ ì •ë³´ ==================
        year = datetime.now().year
        job_id = f"wanted_{year}_{job_data.get('id', '')}"
        job_url = url
        platform = "wanted"
        
        # ================== í”Œëž«í¼ ë° íšŒì‚¬ ì •ë³´ ==================
        job_title = job_detail.get('position', '')
        company = {
            "name": company_data.get('name', ''),
            "size": self._infer_company_size_from_tags(job_data.get('attraction_tags', [])),
            "description": job_detail.get('intro', ''),
            "sales": company_data.get('revenue', ''),
            "industry": company_data.get('industry_name', ''),
            "founded": self._infer_company_founded_date_from_tags(job_data.get('attraction_tags', [])),
        }
        
        # ================== ê³µê³  ê¸°ë³¸ ì •ë³´ ==================
        work_type = self._map_employment_type(job_data.get('employment_type', ''))
        location = {
            "city": address_data.get('location', ''),
            "district": address_data.get('district', ''),
            "detail_address": address_data.get('full_location', '')
        }
        
        # ================== ìžê²© ìš”ê±´ ==================
        requirements_text = job_detail.get('requirements', '')
        #self.logger.debug(f"ðŸ” ìžê²© ìš”ê±´: {requirements_text}")
        education = self._extract_education_from_text(requirements_text)
        experience = self._map_experience_years(job_data)
        
        # ================== í¬ì§€ì…˜ ì •ë³´ ==================
        position_raw = self._extract_position_from_api(job_data.get('category_tag', {}))
        position = normalize_position_data(
            position_raw["raw_text"], 
            position_raw["raw_list"]
        )
        
        # ================== ê¸°ìˆ  ìŠ¤íƒ ì •ë³´ ==================
        tech_stack = self._extract_tech_stack_from_api(job_data.get('skill_tags', []), job_detail)
        
        # ================== ìš°ëŒ€ ê²½í—˜/ì‚¬í•­ ==================
        preferred_experience = self._extract_preferred_experience_from_api(job_detail)
        
        # JobPostingModel ìƒì„± ë° ë°˜í™˜
        job = JobPostingModel.create(
            job_id=job_id,
            job_url=job_url,
            platform=platform,
            job_title=job_title,
            company=company,
            location=location,
            position=position,
            tech_stack=tech_stack,
            experience=experience,
            education=education,
            work_type=work_type,
            preferred_experience=preferred_experience
        )
        
        return job

    # ================== ë°ì´í„° ì¶”ì¶œ í—¬í¼ ë©”ì„œë“œë“¤ ==================
    
    def _extract_position_from_api(self, category_tag: dict) -> dict:
        """APIì—ì„œ í¬ì§€ì…˜ ì›ì‹œ ë°ì´í„° ì¶”ì¶œ"""
        position_texts = []
        
        # parent_tag ì²˜ë¦¬
        parent_tag = category_tag.get('parent_tag', {})
        if parent_tag.get('text'):
            position_texts.append(parent_tag.get('text'))
        
        # child_tags ì²˜ë¦¬
        child_tags = category_tag.get('child_tags', [])
        for tag in child_tags:
            if isinstance(tag, dict) and tag.get('text'):
                tag_text = tag.get('text')
                if tag_text not in position_texts:
                    position_texts.append(tag_text)
        
        return {
            "raw_text": ', '.join(position_texts) if position_texts else '',
            "raw_list": position_texts
        }
    
    def _extract_tech_stack_from_api(self, skill_tags: list, job_detail: dict) -> dict:
        """APIì—ì„œ ê¸°ìˆ ìŠ¤íƒ ì›ì‹œ ë°ì´í„° ì¶”ì¶œ"""
        tech_names = []
        
        # skill_tagsì—ì„œ ê¸°ìˆ ëª… ì¶”ì¶œ
        for tag in skill_tags:
            if isinstance(tag, dict) and tag.get('text'):
                tech_name = tag.get('text')
                if tech_name not in tech_names:
                    tech_names.append(tech_name)
        
        # ì¶”ê°€ ê¸°ìˆ ìŠ¤íƒì„ job_detailì—ì„œ ë³´ì™„ ì¶”ì¶œ
        main_tasks = job_detail.get('main_tasks', '')
        requirements = job_detail.get('requirements', '')
        preferred_points = job_detail.get('preferred_points', '')
        additional_techs = self._extract_techs_from_text(main_tasks + ' ' + requirements + ' ' + preferred_points)
        
        # ì¤‘ë³µ ì œê±°í•˜ë©° í•©ì¹˜ê¸° (skill_tags ìš°ì„ ìˆœìœ„)
        all_techs = tech_names.copy()
        for tech in additional_techs:
            if tech and tech not in all_techs:
                all_techs.append(tech)
        
        return {
            "raw_text": ', '.join(all_techs) if all_techs else '',
            "raw_list": all_techs
        }

    def _extract_preferred_experience_from_api(self, job_detail: dict) -> dict:
        """APIì—ì„œ ìš°ëŒ€ì‚¬í•­ ì›ì‹œ ë°ì´í„° ì¶”ì¶œ"""
        preferred_text = job_detail.get('preferred_points', '')
        
        if not preferred_text:
            return {"raw_text": "", "raw_list": []}
        
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ ì‹œë„
        separators = [r'â€¢\s*', r'\*\s*', r'-\s*', r'\d+\.\s*', r'\n\s*', r';\s*']
        
        raw_list = [preferred_text]  # ê¸°ë³¸ê°’
        
        for sep_pattern in separators:
            parts = re.split(sep_pattern, preferred_text)
            if len(parts) > 1:
                raw_list = [part.strip() for part in parts if part.strip() and len(part.strip()) >= 5]
                break
        
        if not raw_list:
            raw_list = [preferred_text] if preferred_text else []
        
        return {
            "raw_text": preferred_text,
            "raw_list": raw_list
        }

    def _infer_company_size_from_tags(self, attraction_tags: list) -> str:
        """ì–´íŠ¸ëž™ì…˜ íƒœê·¸ì—ì„œ íšŒì‚¬ ê·œëª¨ ì¶”ë¡ """
        size_tag_mapping = {
            10402: "50ëª…ì´í•˜",  # ìŠ¤íƒ€íŠ¸ì—…
            10403: "51-100ëª…",
            10404: "101-500ëª…", 
            10405: "501-1000ëª…",
            10406: "1000ëª…ì´ìƒ"
        }
        
        for tag in attraction_tags:
            if isinstance(tag, dict):
                tag_id = tag.get('tag_type_id')
            else:
                tag_id = tag
                
            if tag_id in size_tag_mapping:
                return size_tag_mapping[tag_id]
        
        return ""

    def _infer_company_founded_date_from_tags(self, attraction_tags: list) -> str:
        """ì–´íŠ¸ëž™ì…˜ íƒœê·¸ì—ì„œ íšŒì‚¬ ì„¤ë¦½ ì—°ì°¨ ì¶”ë¡ """
        founded_tag_mapping = {
            10407: "ì„¤ë¦½3ë…„ì´í•˜",   # ì¶”ì • (10408 ì´ì „)
            10408: "ì„¤ë¦½4~9ë…„", 
            10409: "ì„¤ë¦½10ë…„ì´ìƒ",
        }
        
        for tag in attraction_tags:
            if isinstance(tag, dict):
                tag_id = tag.get('tag_type_id')
            else:
                tag_id = tag
                
            if tag_id in founded_tag_mapping:
                return founded_tag_mapping[tag_id]
        
        return ""

    def _map_employment_type(self, employment_type: str) -> str:
        """ì›í‹°ë“œ ê³ ìš©í˜•íƒœë¥¼ í‘œì¤€ í˜•íƒœë¡œ ë§¤í•‘"""
        mapping = {
            'regular': 'ì •ê·œì§',
            'contract': 'ê³„ì•½ì§',
            'intern': 'ì¸í„´',
            'freelance': 'í”„ë¦¬ëžœì„œ',
            'part_time': 'íŒŒíŠ¸íƒ€ìž„'
        }
        
        return mapping.get(employment_type, employment_type)

    def _map_experience_years(self, job_data: dict) -> dict:
        """ê²½ë ¥ ë…„ìˆ˜ ë§¤í•‘"""
        annual_to = job_data.get('annual_to', 0)
        annual_from = job_data.get('annual_from', 0)
        is_newbie = job_data.get('is_newbie', False)
        
        if is_newbie:
            return {
                "raw_text": "ì‹ ìž…",
                "min_years": 0,
                "max_years": 1
            }
        else:
            raw_text = f"ê²½ë ¥{annual_from}-{annual_to}ë…„" if annual_to < 100 else f"ê²½ë ¥{annual_from}ë…„ ì´ìƒ"
            
            return {
                "raw_text": raw_text,
                "min_years": annual_from,
                "max_years": annual_to 
            }