import time
import re
import random
from bs4 import BeautifulSoup
from datetime import datetime
from time import sleep
from urllib.parse import urlencode
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

from crawler.base_crawler import JobCrawler
from models.data_models import JobPostingModel
from utils.position_normalizer import normalize_position_data

class SaraminCrawler(JobCrawler):
    """
    ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•œ ìµœì í™”ëœ ì‚¬ëŒì¸ í¬ë¡¤ëŸ¬
    """
    
    def __init__(self, site_name: str):
        super().__init__(site_name)
        self.url_metadata = {}
        self.playwright = None
        self.browser = None
        self.browser_context = None
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
        ]
    
    def __enter__(self):
        """ìŠ¤í…”ìŠ¤ ëª¨ë“œ ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        try:
            self.playwright = sync_playwright().start()
            
            # ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•œ ê³ ê¸‰ ì„¤ì •
            self.browser = self.playwright.chromium.launch(
                headless=False,  # headlessë¥¼ Falseë¡œ ë³€ê²½ (íƒì§€ íšŒí”¼)
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-dev-shm-usage',
                    '--disable-extensions',
                    '--disable-plugins',
                    '--disable-images',  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
                    '--disable-javascript',  # JS ë¹„í™œì„±í™” ì˜µì…˜ (í•„ìš”ì‹œ)
                    '--user-agent=' + random.choice(self.user_agents),
                    '--disable-gpu',
                    '--no-first-run',
                    '--no-default-browser-check',
                    '--disable-default-apps',
                    '--disable-popup-blocking',
                    '--disable-translate',
                    '--disable-background-timer-throttling',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-renderer-backgrounding',
                    '--disable-field-trial-config',
                    '--disable-back-forward-cache',
                    '--disable-ipc-flooding-protection'
                ]
            )
            
            # ìŠ¤í…”ìŠ¤ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            self.browser_context = self.browser.new_context(
                user_agent=random.choice(self.user_agents),
                viewport={'width': 720, 'height': 480},  # ì¼ë°˜ì ì¸ í•´ìƒë„
                locale='ko-KR',
                timezone_id='Asia/Seoul',
                permissions=['geolocation'],
                geolocation={'latitude': 37.5665, 'longitude': 126.9780},  # ì„œìš¸ ì¢Œí‘œ
                color_scheme='light',
                reduced_motion='no-preference',
                forced_colors='none',
                # ì¶”ê°€ ìŠ¤í…”ìŠ¤ ì„¤ì •
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
                    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-User': '?1',
                    'Sec-Fetch-Dest': 'document',
                    'Cache-Control': 'max-age=0'
                }
            )
            
            # ë´‡ íƒì§€ ìŠ¤í¬ë¦½íŠ¸ ì°¨ë‹¨
            self._inject_stealth_scripts()
            
            return self
            
        except Exception as e:
            self.logger.error(f"âŒ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._cleanup_browser()
            raise
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì•ˆì „í•œ ë¸Œë¼ìš°ì € ì •ë¦¬"""
        self._cleanup_browser()
    
    def _cleanup_browser(self):
        """ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.browser_context:
                # ëª¨ë“  í˜ì´ì§€ ë¨¼ì € ë‹«ê¸°
                for page in self.browser_context.pages:
                    try:
                        page.close()
                    except:
                        pass
                time.sleep(0.5)
                self.browser_context.close()
                self.browser_context = None
                
            if self.browser:
                time.sleep(0.5)
                self.browser.close()
                self.browser = None
                
            if self.playwright:
                time.sleep(0.5)
                self.playwright.stop()
                self.playwright = None
                
            self.logger.debug("âœ… ë¸Œë¼ìš°ì € ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë¸Œë¼ìš°ì € ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _inject_stealth_scripts(self):
        """ë´‡ íƒì§€ íšŒí”¼ ìŠ¤í¬ë¦½íŠ¸ ì£¼ì…"""
        try:
            # WebDriver ì†ì„± ìˆ¨ê¸°ê¸°
            stealth_script = """
            // WebDriver ì†ì„± ì œê±°
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            
            // Chrome ì†ì„± ì¶”ê°€
            window.chrome = {
                runtime: {},
            };
            
            // Permissions API ëª¨í‚¹
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            // Plugin array ëª¨í‚¹
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            
            // Languages ì„¤ì •
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ko-KR', 'ko', 'en-US', 'en'],
            });
            """
            
            # ëª¨ë“  í˜ì´ì§€ì— ìŠ¤í¬ë¦½íŠ¸ ì£¼ì…
            self.browser_context.add_init_script(stealth_script)
            
        except Exception as e:
            self.logger.error(f"âŒ ìŠ¤í…”ìŠ¤ ìŠ¤í¬ë¦½íŠ¸ ì£¼ì… ì‹¤íŒ¨: {e}")
    
    def _load_page_with_playwright(self, url: str, max_retries: int = 3) -> str:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ì•ˆì „í•œ í˜ì´ì§€ ë¡œë“œ"""
        for attempt in range(max_retries):
            try:
                # í˜ì´ì§€ ìƒì„±
                page = self.browser_context.new_page()
                
                # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ìœ¼ë¡œ ì†ë„ í–¥ìƒ
                page.route("**/*.{png,jpg,jpeg,gif,svg,css,woff,woff2}", lambda route: route.abort())
                
                self.logger.debug(f"ğŸŒ í˜ì´ì§€ ë¡œë”© ì‹œë„ {attempt + 1}/{max_retries}: {url}")
                
                # ëœë¤ ë”œë ˆì´ (ë´‡ íƒì§€ íšŒí”¼)
                if attempt > 0:
                    delay = random.uniform(2.0, 5.0)
                    self.logger.debug(f"â³ ì¬ì‹œë„ ì „ ëŒ€ê¸°: {delay:.1f}ì´ˆ")
                    time.sleep(delay)
                
                # í˜ì´ì§€ ë¡œë“œ
                response = page.goto(
                    url, 
                    wait_until='domcontentloaded',  # networkidle ëŒ€ì‹  ë” ë¹ ë¥¸ ì˜µì…˜
                    timeout=20000  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
                )
                
                # ì‘ë‹µ ìƒíƒœ í™•ì¸
                if response and response.status >= 400:
                    self.logger.warning(f"âš ï¸ HTTP {response.status} ì‘ë‹µ: {url}")
                    if response.status == 403:
                        self.logger.error("ğŸš« 403 Forbidden - ë´‡ íƒì§€ë¨")
                        page.close()
                        # ë” ê¸´ ëŒ€ê¸° í›„ ì¬ì‹œë„
                        time.sleep(random.uniform(10.0, 15.0))
                        continue
                
                # í˜ì´ì§€ ì™„ì „ ë¡œë”© ëŒ€ê¸°
                try:
                    # í•µì‹¬ ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                    page.wait_for_selector('body', timeout=10000)
                    
                    # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ë¡œë”©)
                    random_wait = random.uniform(1.5, 3.0)
                    page.wait_for_timeout(int(random_wait * 1000))
                    
                except PlaywrightTimeoutError:
                    self.logger.warning("âš ï¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ - ê³„ì† ì§„í–‰")
                
                # HTML ì½˜í…ì¸  ì¶”ì¶œ
                html_content = page.content()
                page.close()
                
                # ì„±ê³µì ìœ¼ë¡œ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ í™•ì¸
                if len(html_content) > 1000:  # ìµœì†Œ ì½˜í…ì¸  ê¸¸ì´ ì²´í¬
                    self.logger.debug(f"âœ… í˜ì´ì§€ ë¡œë”© ì„±ê³µ: {len(html_content)} ê¸€ì")
                    return html_content
                else:
                    self.logger.warning(f"âš ï¸ ì½˜í…ì¸  ë¶€ì¡±: {len(html_content)} ê¸€ì")
                    
            except PlaywrightTimeoutError:
                self.logger.warning(f"âš ï¸ ì‹œë„ {attempt + 1} íƒ€ì„ì•„ì›ƒ: {url}")
                try:
                    page.close()
                except:
                    pass
                    
            except Exception as e:
                self.logger.error(f"âŒ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                try:
                    page.close()
                except:
                    pass
        
        self.logger.error(f"âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨: {url}")
        return None
    
    def _load_iframe_content_with_playwright(self, iframe_url: str) -> BeautifulSoup:
        """iframe ì „ìš© ìµœì í™”ëœ ë¡œë”©"""
        try:
            # iframeì€ ë” ê´€ëŒ€í•œ ì„¤ì •ìœ¼ë¡œ
            page = self.browser_context.new_page()
            
            # ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ (CSS, ì´ë¯¸ì§€ ë“±)
            page.route("**/*.{png,jpg,jpeg,gif,svg,css,woff,woff2,js}", lambda route: route.abort())
            
            self.logger.debug(f"ğŸ–¼ï¸ iframe ë¡œë”©: {iframe_url}")
            
            # ë” ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
            page.goto(iframe_url, wait_until='domcontentloaded', timeout=15000)
            
            # ìµœì†Œí•œì˜ ëŒ€ê¸°
            page.wait_for_timeout(1000)
            
            try:
                # user_contentê°€ ìˆëŠ”ì§€ í™•ì¸
                page.wait_for_selector('div.user_content', timeout=5000)
            except:
                self.logger.debug("âš ï¸ user_content ë¡œë”© ì§€ì—°")
            
            iframe_html = page.content()
            page.close()
            
            soup = BeautifulSoup(iframe_html, 'html.parser')
            user_content_div = soup.find('div', class_='user_content')
            
            if user_content_div:
                content_text = user_content_div.get_text(strip=True)
                self.logger.debug(f"âœ… iframe ì½˜í…ì¸ : {len(content_text)} ê¸€ì")
                return user_content_div
            else:
                self.logger.warning(f"âš ï¸ user_content ì—†ìŒ: {iframe_url}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ iframe ë¡œë”© ì‹¤íŒ¨: {e}")
            try:
                page.close()
            except:
                pass
            return None
    
    def _get_job_urls(self, config, full_crawl: bool = False) -> list:
        """ëª©ë¡ í˜ì´ì§€ëŠ” ê¸°ì¡´ requests ë°©ì‹ ìœ ì§€ (ë™ì  ë¡œë”© ì˜í–¥ ì ìŒ)"""
        all_job_urls = []
        
        base_url = "https://www.saramin.co.kr/zf_user/jobs/list/job-category"
        base_params = {
            'cat_mcls': '2',
            'search_done': 'y',
            'page_count': '50',
            'sort': 'RD',
            'type': 'job-category'
        }
        
        self.session.headers.update(config.common_headers)
        max_pages = 10 if full_crawl else config.max_pages
        consecutive_empty = 0
        
        self.logger.info(f"ğŸ¯ ì‚¬ëŒì¸ IT ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        try:
            for page_num in range(1, max_pages + 1):
                try:
                    params = base_params.copy()
                    params['page'] = str(page_num)
                    
                    if page_num > 1:
                        sleep(random.uniform(2, 4))
                    
                    response = self.session.get(base_url, params=params, timeout=30) 
                    response.raise_for_status()
                    self.logger.info(f"ğŸ“„ ì‚¬ëŒì¸ page {page_num} ì²˜ë¦¬ ì¤‘...")
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    page_urls = self._extract_urls_from_current_page(soup)
                    
                    if not page_urls:
                        consecutive_empty += 1
                        self.logger.info(f"ğŸ“„ ë¹ˆ í˜ì´ì§€ ({consecutive_empty}/3)")
                        
                        if consecutive_empty >= 3:
                            self.logger.info("ğŸ›‘ ì—°ì† ë¹ˆ í˜ì´ì§€ë¡œ ì¸í•œ ìˆ˜ì§‘ ì¢…ë£Œ")
                            break
                    else:
                        consecutive_empty = 0
                        all_job_urls.extend(page_urls)
                        self.logger.info(f"ğŸ“„ page {page_num}: {len(page_urls)}ê°œ URL ìˆ˜ì§‘")
                    
                except Exception as e:
                    self.logger.error(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    consecutive_empty += 1
                    if consecutive_empty >= 3:
                        break
        
        except Exception as e:
            self.logger.error(f"âŒ ì‚¬ëŒì¸ URL ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        
        unique_urls = list(set(all_job_urls))
        self.logger.info(f"ğŸ¯ ì‚¬ëŒì¸ ì „ì²´ URL ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_urls)}ê°œ ê³ ìœ  URL")
        
        return unique_urls

    def _extract_urls_from_current_page(self, soup: BeautifulSoup) -> list:
        """ê¸°ì¡´ URL ì¶”ì¶œ ë¡œì§ ìœ ì§€"""
        job_urls = []
        
        try:
            recruiting_section = soup.find('section', class_='list_recruiting')
            if not recruiting_section:
                self.logger.warning("âš ï¸ list_recruiting ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            list_items = recruiting_section.find_all('div', class_='list_item')
            list_items = list_items[:30]
            
            for item in list_items:
                try:
                    job_link = self._extract_job_link_from_item(item)
                    if not job_link:
                        continue
                    
                    href = job_link.get('href')
                    if not href:
                        continue
                    
                    if href.startswith('/'):
                        full_url = f"https://www.saramin.co.kr{href}"
                    else:
                        full_url = href
                    
                    if not self._is_valid_saramin_job_url(full_url):
                        continue
                    
                    job_urls.append(full_url)
                    
                    metadata = self._extract_metadata_from_item(item, full_url)
                    self.url_metadata[full_url] = metadata
                    
                    company = metadata.get('company_name', 'Unknown')
                    title = metadata.get('position_title', 'Unknown')
                    self.logger.info(f"âœ… [FOUND] ì±„ìš©ê³µê³  | íšŒì‚¬: {company} | {title}")
                    
                except Exception:
                    continue
            
        except Exception as e:
            self.logger.error(f"âŒ URL ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return job_urls

    def _extract_job_link_from_item(self, item):
        """ê¸°ì¡´ ë§í¬ ì¶”ì¶œ ë¡œì§ ìœ ì§€"""
        job_tit = item.find('div', class_='job_tit')
        if job_tit:
            job_link = job_tit.find('a', class_='str_tit')
            if job_link:
                return job_link
        
        job_link = item.find('a', href=re.compile(r'/zf_user/jobs/relay/view'))
        if job_link:
            return job_link
        
        job_link = item.find('a', href=re.compile(r'/zf_user/jobs/view'))
        if job_link:
            return job_link
        
        return None

    def _is_valid_saramin_job_url(self, url: str) -> bool:
        """ê¸°ì¡´ URL ê²€ì¦ ë¡œì§ ìœ ì§€"""
        return ('saramin.co.kr' in url and 
                ('jobs/relay/view' in url or 'jobs/view' in url))

    def _extract_metadata_from_item(self, item, job_url: str) -> dict:
        """ê³µê³  ì•„ì´í…œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {
            'job_url': job_url,
            'company_name': '',
            'position_title': '',
            'location': '',
            'career': '',
            'education': '',
            'deadline': '',
            'registration_date': '',
            'job_sectors': [],
            'badges': [],
            'company_group': '',
            'company_type': '',
            'rec_idx': ''
        }
        
        try:
            # rec_idx ì¶”ì¶œ (ì‹ë³„ìš©)
            rec_match = re.search(r'rec_idx[=:](\d+)', job_url)
            if rec_match:
                metadata['rec_idx'] = rec_match.group(1)
            
            # íšŒì‚¬ëª… ì¶”ì¶œ
            company_elem = item.find('div', class_=lambda x: x and 'company' in x)
            if company_elem:
                company_link = company_elem.find('a')
                if company_link:
                    metadata['company_name'] = company_link.get_text(strip=True)
                else:
                    metadata['company_name'] = company_elem.get_text(strip=True)
            
            # ê³µê³  ì œëª© ì¶”ì¶œ
            title_elem = item.find('div', class_=lambda x: x and ('job' in x or 'title' in x))
            if title_elem:
                title_link = title_elem.find('a')
                if title_link:
                    title_text = (title_link.get('title') or 
                                 title_link.get_text(strip=True))
                    metadata['position_title'] = title_text
            
            # ì§ë¬´ ë¶„ì•¼ ì¶”ì¶œ
            self._extract_job_sectors(item, metadata)
            
            # ë±ƒì§€ ì •ë³´ ì¶”ì¶œ
            self._extract_badges(item, metadata)
            
        except Exception as e:
            self.logger.debug(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return metadata
        
    def _extract_job_sectors(self, item, metadata):
        """ì§ë¬´ ë¶„ì•¼ ì¶”ì¶œ"""
        try:
            sector_elements = item.find_all(['span', 'div'], 
                                          class_=lambda x: x and 'sector' in x)
            for elem in sector_elements:
                sectors = [span.get_text(strip=True) for span in elem.find_all('span')]
                metadata['job_sectors'].extend(sectors)
        except Exception:
            pass

    def _extract_badges(self, item, metadata):
        """ë±ƒì§€ ì •ë³´ ì¶”ì¶œ"""
        try:
            badge_elements = item.find_all(['span', 'div'], 
                                         class_=lambda x: x and 'badge' in x)
            for elem in badge_elements:
                badges = [span.get_text(strip=True) for span in elem.find_all('span')]
                for badge in badges:
                    if badge and len(badge) < 20:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸
                        metadata['badges'].append(badge)
        except Exception:
            pass

    
    def _crawl_job_detail(self, url: str, config) -> JobPostingModel:
        """ë´‡ íƒì§€ íšŒí”¼ê°€ í¬í•¨ëœ ìƒì„¸ í¬ë¡¤ë§"""
        try:
            rec_match = re.search(r'rec_idx[=:](\d+)', url)
            if not rec_match:
                self.logger.error(f"âŒ rec_idx ì¶”ì¶œ ì‹¤íŒ¨: {url}")
                return None
            
            rec_idx = rec_match.group(1)
            self.logger.info(f"ğŸ” ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘: rec_idx={rec_idx}")
            
            # ëœë¤ ë”œë ˆì´ (ë´‡ íƒì§€ íšŒí”¼)
            delay = random.uniform(1.0, 3.0)
            self.logger.debug(f"â³ í¬ë¡¤ë§ ì „ ëŒ€ê¸°: {delay:.1f}ì´ˆ")
            time.sleep(delay)
            
            # ì•ˆì „í•œ í˜ì´ì§€ ë¡œë”©
            html_content = self._load_page_with_playwright(url)
            if not html_content:
                self.logger.error(f"âŒ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {url}")
                return None
            
            # íŒŒì‹± ì§„í–‰
            soup = BeautifulSoup(html_content, 'html.parser')
            job_section = self._get_job_specific_section(soup, rec_idx)
            
            if not job_section:
                self.logger.error(f"âŒ ê³µê³  ì„¹ì…˜ ì—†ìŒ: {rec_idx}")
                return None
            
            job_posting = self._parse_saramin_detail_to_jobposting(job_section, url, rec_idx)
            
            # ì„±ê³µ í›„ ì¶”ê°€ ë”œë ˆì´
            success_delay = random.uniform(0.5, 1.5)
            time.sleep(success_delay)
            
            return job_posting
            
        except Exception as e:
            self.logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            return None

    def _get_job_specific_section(self, soup: BeautifulSoup, rec_idx: str) -> BeautifulSoup:
        """ê³µê³ ë³„ ê³ ìœ  ì„¹ì…˜ ì¶”ì¶œ"""
        try:
            self.logger.debug(f"ğŸ” ê³µê³ ë³„ ì„¹ì…˜ íƒìƒ‰ ì‹œì‘: rec_idx={rec_idx}")
            
            all_sections = soup.find_all('section', class_='jview')
            self.logger.debug(f"ğŸ“‹ ì „ì²´ jview ì„¹ì…˜ ìˆ˜: {len(all_sections)}")
            
            target_class = f"jview-0-{rec_idx}"
            job_section = soup.find('section', class_=lambda x: x and 'jview' in x and target_class in x)
            
            if job_section:
                self.logger.debug(f"âœ… ê³µê³  ì„¹ì…˜ ë°œê²¬: {job_section.get('class')}")
                return job_section
            
            for section in all_sections:
                class_list = section.get('class', [])
                for class_name in class_list:
                    if rec_idx in class_name and 'jview-' in class_name:
                        self.logger.debug(f"âœ… ê³µê³  ì„¹ì…˜ ë°œê²¬ (íŒ¨í„´2): {class_list}")
                        return section
            
            self.logger.warning(f"âš ï¸ ê³µê³ ë³„ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ í˜ì´ì§€ ì‚¬ìš©: {rec_idx}")
            return soup
            
        except Exception as e:
            self.logger.error(f"âŒ ê³µê³  ì„¹ì…˜ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return soup

    def _parse_saramin_detail_to_jobposting(self, job_section: BeautifulSoup, url: str, rec_idx: str) -> JobPostingModel:
        """
        ì‚¬ëŒì¸ ìƒì„¸ í˜ì´ì§€ ì¢…í•© íŒŒì‹± - jv_summary ì„¹ì…˜ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        """
        # ================== ê¸°ë³¸ ì‹ë³„ ì •ë³´ ==================
        year = datetime.now().year
        full_job_id = f"saramin_{year}_{rec_idx}"
        platform = "saramin"
        job_url = url
        
        # URL ë©”íƒ€ë°ì´í„°ì—ì„œ ê¸°ë³¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        metadata = self.url_metadata.get(url, {})
        
        # ================== jv_summary ì„¹ì…˜ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ ==================
        summary_info = self._extract_summary_info(job_section)
        
        # ================== í”Œë«í¼ ë° íšŒì‚¬ ì •ë³´ ==================
        job_title = self._extract_job_title(metadata, job_section)
        company_name = self._extract_company_name(metadata, job_section)
        company = self._extract_company_info(job_section, company_name, metadata)
        
        # ================== ê³µê³  ê¸°ë³¸ ì •ë³´ (summaryì—ì„œ ìš°ì„  ì¶”ì¶œ) ==================
        work_type = summary_info.get('work_type') or self._extract_work_type(metadata, job_section)
        location = summary_info.get('location') or self._extract_location_info(metadata, job_section)
        
        # ================== ìê²© ìš”ê±´ (summaryì—ì„œ ìš°ì„  ì¶”ì¶œ) ==================
        education = summary_info.get('education') or self._extract_education(metadata, job_section)
        experience = summary_info.get('experience') or self._extract_experience_info(metadata, job_section)
        
        # ================== ì§ë¬´ ì •ë³´ ==================
        position = self._extract_position_info(metadata, job_section)
        
        # ================== ê¸°ìˆ  ìŠ¤íƒ ì •ë³´ ==================
        tech_stack = self._extract_tech_stack(job_section)
        
        # ================== ìš°ëŒ€ ê²½í—˜/ì‚¬í•­ ==================
        preferred_experience = self._extract_preferred_experience(job_section)
        
        job_posting = JobPostingModel.create(
            job_id=full_job_id,
            job_url=job_url,
            platform=platform,
            job_title=job_title,
            company={
                "name": company_name,
                "size": company.get('size', ''),
                "description": company.get('description', ''),
                "sales": company.get('sales', ''),
                "industry": company.get('industry', '')
            },
            location=location,
            position=position,
            tech_stack=tech_stack,
            experience=experience,
            education=education,
            work_type=work_type,
            preferred_experience=preferred_experience
        )
        
        return job_posting

    def _extract_summary_info(self, job_section: BeautifulSoup) -> dict:
        """
        jv_summary ì„¹ì…˜ì—ì„œ í•µì‹¬ ì •ë³´ ì¼ê´„ ì¶”ì¶œ
        """
        summary_info = {
            'work_type': '',
            'location': {},
            'education': '',
            'experience': {}
        }
        
        try:
            # jv_summary ì„¹ì…˜ ì°¾ê¸°
            summary_section = job_section.find('div', class_='jv_summary')
            if not summary_section:
                self.logger.debug("âš ï¸ jv_summary ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return summary_info
            
            # dl/dt/dd êµ¬ì¡°ì—ì„œ ì •ë³´ ì¶”ì¶œ
            dl_elements = summary_section.find_all('dl')
            
            for dl in dl_elements:
                dt = dl.find('dt')
                dd = dl.find('dd')
                
                if not dt or not dd:
                    continue
                    
                key = dt.get_text(strip=True)
                value = dd.get_text(strip=True)
                
                # ê²½ë ¥ ì •ë³´ ì¶”ì¶œ
                if key == 'ê²½ë ¥':
                    summary_info['experience'] = self._parse_experience(value)
                    self.logger.debug(f"âœ… summaryì—ì„œ ê²½ë ¥ ì¶”ì¶œ: {value}")
                
                # í•™ë ¥ ì •ë³´ ì¶”ì¶œ  
                elif key == 'í•™ë ¥':
                    summary_info['education'] = value
                    self.logger.debug(f"âœ… summaryì—ì„œ í•™ë ¥ ì¶”ì¶œ: {value}")
                
                # ê·¼ë¬´í˜•íƒœ ì¶”ì¶œ
                elif key == 'ê·¼ë¬´í˜•íƒœ':
                    summary_info['work_type'] = value
                    self.logger.debug(f"âœ… summaryì—ì„œ ê·¼ë¬´í˜•íƒœ ì¶”ì¶œ: {value}")
                
                # ê·¼ë¬´ì§€ì—­ ì¶”ì¶œ
                elif key == 'ê·¼ë¬´ì§€ì—­':
                    summary_info['location'] = self._normalize_location(value)
                    self.logger.debug(f"âœ… summaryì—ì„œ ê·¼ë¬´ì§€ì—­ ì¶”ì¶œ: {value}")
                    
        except Exception as e:
            self.logger.error(f"âŒ summary ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return summary_info

    # ================== ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ ==================
    
    def _extract_tech_stack(self, job_section: BeautifulSoup) -> dict:
        """ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ - ë‹¤ë‹¨ê³„ fallback ì²˜ë¦¬"""
        tech_stack = {"raw_text": "", "raw_list": []}
        
        try:
            # 1. ìš°ì„  ëª…ì‹œì  ê¸°ìˆ  íƒœê·¸ì—ì„œ ì‹œë„
            tech_section = job_section.find('div', class_='job_skill')
            if tech_section:
                tech_items = tech_section.find_all(['span', 'div'], class_='skill')
                tech_list = [item.get_text(strip=True) for item in tech_items if item.get_text(strip=True)]
                
                if tech_list:
                    tech_stack['raw_list'] = tech_list
                    tech_stack['raw_text'] = ', '.join(tech_list)
                    self.logger.debug(f"âœ… ëª…ì‹œì  ê¸°ìˆ  íƒœê·¸ì—ì„œ ì¶”ì¶œ: {tech_list}")
                    return tech_stack
            
            # 2. iframe ë‚´ë¶€ user_contentì—ì„œ ì¶”ì¶œ ì‹œë„
            iframe_techs = self._extract_tech_stack_from_iframe(job_section)
            if iframe_techs['raw_list']:
                self.logger.debug(f"âœ… iframe user_contentì—ì„œ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ: {len(iframe_techs['raw_list'])}ê°œ")
                return iframe_techs
            
            # 3. iframe ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ì™¸ë¶€ í…ìŠ¤íŠ¸ì—ì„œ fallback ì¶”ì¶œ
            self.logger.debug("âš ï¸ iframe ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ ì‹¤íŒ¨ - ì™¸ë¶€ í…ìŠ¤íŠ¸ì—ì„œ fallback ì‹œë„")
            
            # 3-1. ì „ì²´ jview ì„¹ì…˜ì—ì„œ ì¶”ì¶œ
            full_text = job_section.get_text()
            extracted_techs = self._extract_techs_from_text(full_text)
            
            if extracted_techs:
                tech_stack['raw_list'] = list(set(extracted_techs))
                tech_stack['raw_text'] = ', '.join(tech_stack['raw_list'])
                self.logger.debug(f"âœ… ì™¸ë¶€ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ: {len(tech_stack['raw_list'])}ê°œ")
                return tech_stack
            
            # 3-2. job_description í´ë˜ìŠ¤ì—ì„œ ì‹œë„
            job_content = job_section.find('div', class_='job_description')
            if job_content:
                content_text = job_content.get_text()
                extracted_techs = self._extract_techs_from_text(content_text)
                
                if extracted_techs:
                    tech_stack['raw_list'] = list(set(extracted_techs))
                    tech_stack['raw_text'] = ', '.join(tech_stack['raw_list'])
                    self.logger.debug(f"âœ… job_descriptionì—ì„œ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ: {len(tech_stack['raw_list'])}ê°œ")
                    return tech_stack
            
            self.logger.warning("âš ï¸ ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ ì‹¤íŒ¨")
                        
        except Exception as e:
            self.logger.error(f"âŒ ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return tech_stack

    def _extract_tech_stack_from_iframe(self, job_section: BeautifulSoup) -> dict:
        """iframe ë‚´ë¶€ user_contentì—ì„œ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ"""
        tech_stack = {"raw_text": "", "raw_list": []}
        
        try:
            # iframe ì°¾ê¸°
            iframe_element = job_section.find('iframe', class_='iframe_content')
            if not iframe_element:
                self.logger.debug("âš ï¸ iframe_contentë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return tech_stack
            
            iframe_src = iframe_element.get('src')
            if not iframe_src:
                self.logger.debug("âš ï¸ iframe srcê°€ ì—†ìŒ")
                return tech_stack
            
            # iframe URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
            if iframe_src.startswith('/'):
                iframe_url = f"https://www.saramin.co.kr{iframe_src}"
            else:
                iframe_url = iframe_src
            
            # Playwrightë¡œ iframe ë‚´ìš© ë¡œë“œ
            user_content_soup = self._load_iframe_content_with_playwright(iframe_url)
            if not user_content_soup:
                self.logger.warning(f"âš ï¸ iframe ë‚´ìš© ë¡œë“œ ì‹¤íŒ¨ - ì™¸ë¶€ fallback ì‹œë„: {iframe_url}")
                return tech_stack
            
            # user_content ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ
            content_text = user_content_soup.get_text(strip=True)
            
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì˜ë¯¸ìˆëŠ” ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œì´ ì–´ë ¤ì›€
            if len(content_text) < 30:
                self.logger.warning(f"âš ï¸ iframe í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë¶€ì¡± ({len(content_text)}ê¸€ì) - ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ í¬ê¸°")
                return tech_stack
            
            extracted_techs = self._extract_techs_from_text(content_text)
            
            if extracted_techs:
                tech_stack['raw_list'] = list(set(extracted_techs))
                tech_stack['raw_text'] = ', '.join(tech_stack['raw_list'])
                self.logger.debug(f"âœ… iframeì—ì„œ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ ì™„ë£Œ: {len(tech_stack['raw_list'])}ê°œ")
            else:
                self.logger.debug(f"âš ï¸ iframeì—ì„œ ê¸°ìˆ ìŠ¤íƒ ë¯¸ë°œê²¬ - ì½˜í…ì¸  ê¸¸ì´: {len(content_text)}ê¸€ì")
            
        except Exception as e:
            self.logger.error(f"âŒ iframe ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return tech_stack

    # ================== ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ ==================
    
    def _extract_preferred_experience(self, job_section: BeautifulSoup) -> dict:
        """iframe user_contentì—ì„œ ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ - ë™ì  ë¡œë”© ëŒ€ì‘"""
        preferred = {"raw_text": "", "raw_list": []}
        
        try:
            # 1. ìš°ì„  ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
            preferred_section = job_section.find('div', class_='preferred')
            if preferred_section:
                preferred_text = preferred_section.get_text(strip=True)
                if preferred_text:
                    preferred['raw_text'] = preferred_text
                    
                    if 'â€¢' in preferred_text:
                        preferred_list = [item.strip() for item in preferred_text.split('â€¢') if item.strip()]
                    elif '-' in preferred_text:
                        preferred_list = [item.strip() for item in preferred_text.split('-') if item.strip()]
                    else:
                        preferred_list = [preferred_text] if preferred_text else []
                    
                    preferred['raw_list'] = preferred_list
                    self.logger.debug(f"âœ… ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ: {len(preferred_list)}ê°œ")
                    return preferred
            
            # 2. iframe ë‚´ë¶€ user_contentì—ì„œ ì¶”ì¶œ ì‹œë„
            iframe_preferred = self._extract_preferred_experience_from_iframe(job_section)
            if iframe_preferred['raw_text']:
                self.logger.info(f"âœ… iframe user_contentì—ì„œ ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ: {len(iframe_preferred['raw_list'])}ê°œ")
                return iframe_preferred
                    
        except Exception as e:
            self.logger.error(f"âŒ ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return preferred

    def _extract_preferred_experience_from_iframe(self, job_section: BeautifulSoup) -> dict:
        """iframe ë‚´ë¶€ user_contentì—ì„œ ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ"""
        preferred = {"raw_text": "", "raw_list": []}
        
        try:
            # iframe ì°¾ê¸°
            iframe_element = job_section.find('iframe', class_='iframe_content')
            if not iframe_element:
                return preferred
            
            iframe_src = iframe_element.get('src')
            if not iframe_src:
                return preferred
            
            if iframe_src.startswith('/'):
                iframe_url = f"https://www.saramin.co.kr{iframe_src}"
            else:
                iframe_url = iframe_src
            
            # Playwrightë¡œ iframe ë‚´ìš© ë¡œë“œ
            user_content_soup = self._load_iframe_content_with_playwright(iframe_url)
            if not user_content_soup:
                self.logger.warning(f"âš ï¸ iframe ë‚´ìš© ë¡œë“œ ì‹¤íŒ¨ - ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ í¬ê¸°: {iframe_url}")
                return preferred
            
            # í…ìŠ¤íŠ¸ ì½˜í…ì¸  í™•ì¸
            content_text = user_content_soup.get_text(strip=True)
            if len(content_text) < 30:
                self.logger.warning(f"âš ï¸ iframe í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë¶€ì¡± ({len(content_text)}ê¸€ì) - ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ í¬ê¸°")
                return preferred
            
            # user_contentì—ì„œ ìš°ëŒ€ì‚¬í•­ ì„¹ì…˜ ì°¾ê¸°
            preferred_content = self._extract_preferred_from_job_sections(user_content_soup)
            
            if preferred_content['raw_text']:
                preferred = preferred_content
                self.logger.debug(f"âœ… iframeì—ì„œ ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ ì™„ë£Œ: {len(preferred['raw_list'])}ê°œ")
            else:
                self.logger.debug(f"âš ï¸ iframeì—ì„œ ìš°ëŒ€ì‚¬í•­ ë¯¸ë°œê²¬ - ì½˜í…ì¸  ê¸¸ì´: {len(content_text)}ê¸€ì")
            
        except Exception as e:
            self.logger.error(f"âŒ iframe ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return preferred

    def _extract_preferred_from_job_sections(self, user_content_soup) -> dict:
        """user_content ë‚´ì—ì„œ ìš°ëŒ€ì‚¬í•­ ì„¹ì…˜ ì¶”ì¶œ"""
        preferred = {"raw_text": "", "raw_list": []}
        
        try:
            definition_lists = user_content_soup.find_all('dl')
            
            for dl in definition_lists:
                dt_element = dl.find('dt')
                dd_element = dl.find('dd')
                
                if dt_element and dd_element:
                    section_title = dt_element.get_text(strip=True)
                    
                    if 'ìš°ëŒ€ì‚¬í•­' in section_title or 'ìš°ëŒ€ì¡°ê±´' in section_title:
                        section_content = dd_element.get_text(strip=True)
                        
                        if section_content:
                            preferred['raw_text'] = section_content
                            preferred_list = []
                            
                            # pre íƒœê·¸ ë‚´ìš© ìš°ì„  ì²˜ë¦¬
                            pre_elements = dd_element.find_all('pre')
                            for pre in pre_elements:
                                pre_text = pre.get_text(strip=True)
                                
                                if 'â€¢' in pre_text:
                                    items = [item.strip() for item in pre_text.split('â€¢') if item.strip()]
                                elif 'â—¦' in pre_text:
                                    items = [item.strip() for item in pre_text.split('â—¦') if item.strip()]
                                elif re.search(r'\n\s*-\s*', pre_text):
                                    items = [item.strip() for item in re.split(r'\n\s*-\s*', pre_text) if item.strip()]
                                elif '\n' in pre_text:
                                    items = [item.strip() for item in pre_text.split('\n') if item.strip() and len(item.strip()) > 5]
                                else:
                                    items = [pre_text] if pre_text else []
                                
                                preferred_list.extend(items)
                            
                            # pre íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                            if not preferred_list:
                                if 'â€¢' in section_content:
                                    preferred_list = [item.strip() for item in section_content.split('â€¢') if item.strip()]
                                elif 'â—¦' in section_content:
                                    preferred_list = [item.strip() for item in section_content.split('â—¦') if item.strip()]
                                else:
                                    preferred_list = [section_content] if section_content else []
                            
                            preferred['raw_list'] = preferred_list
                            return preferred
            
        except Exception as e:
            self.logger.error(f"âŒ ìš°ëŒ€ì‚¬í•­ ì„¹ì…˜ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return preferred
    # ================== í•µì‹¬ íŒŒì‹± ë¡œì§ ==================
    
    def _load_iframe_content_with_playwright(self, iframe_url: str) -> BeautifulSoup:
        """Playwrightë¡œ iframe ë‚´ìš© ë¡œë“œí•˜ê³  user_content ë°˜í™˜"""
        try:
            page = self.browser_context.new_page()
            
            self.logger.debug(f"ğŸŒ iframe ë¡œë”©: {iframe_url}")
            page.goto(iframe_url, wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(2000)
            
            try:
                page.wait_for_selector('div.user_content', timeout=10000)
            except:
                self.logger.debug("âš ï¸ user_content ë¡œë”© íƒ€ì„ì•„ì›ƒ")
            
            iframe_html = page.content()
            page.close()
            
            soup = BeautifulSoup(iframe_html, 'html.parser')
            
            # PDF/ì´ë¯¸ì§€ ì½˜í…ì¸  ê°ì§€
            self._detect_non_text_content(soup, iframe_url)
            
            user_content_div = soup.find('div', class_='user_content')
            
            if user_content_div:
                content_text = user_content_div.get_text(strip=True)
                self.logger.debug(f"âœ… user_content ë°œê²¬, ê¸¸ì´: {len(content_text)} ê¸€ì")
                
                # í…ìŠ¤íŠ¸ ì½˜í…ì¸ ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì¼€ì´ìŠ¤ë¡œ ë¡œê¹…
                if len(content_text) < 50:
                    self.logger.warning(f"ğŸš¨ [SUSPICIOUS] í…ìŠ¤íŠ¸ ì½˜í…ì¸ ê°€ ë§¤ìš° ì ìŒ ({len(content_text)}ê¸€ì): {iframe_url}")
                    self._log_suspicious_content(iframe_url, content_text, "í…ìŠ¤íŠ¸_ë¶€ì¡±")
                
                return user_content_div
            else:
                self.logger.warning(f"âš ï¸ user_content divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {iframe_url}")
                self._log_suspicious_content(iframe_url, "", "user_content_ì—†ìŒ")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ iframe ë‚´ìš© ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None

    def _detect_non_text_content(self, soup: BeautifulSoup, iframe_url: str):
        """PDF, ì´ë¯¸ì§€ ë“± ë¹„í…ìŠ¤íŠ¸ ì½˜í…ì¸  ê°ì§€ ë° ë¡œê¹…"""
        try:
            # PDF ì„ë² ë“œ ê°ì§€
            pdf_elements = soup.find_all(['embed', 'object', 'iframe'], 
                                       src=lambda x: x and ('.pdf' in x.lower() or 'pdf' in x.lower()))
            if pdf_elements:
                self.logger.warning(f"ğŸš¨ [PDF_DETECTED] PDF ì½˜í…ì¸  ê°ì§€: {iframe_url}")
                self._log_suspicious_content(iframe_url, str(pdf_elements), "PDF_ì½˜í…ì¸ ")
                return
            
            # ì´ë¯¸ì§€ ìœ„ì£¼ ì½˜í…ì¸  ê°ì§€
            img_elements = soup.find_all('img')
            if len(img_elements) > 3:  # ì´ë¯¸ì§€ê°€ ë§ìœ¼ë©´ ì´ë¯¸ì§€ ê¸°ë°˜ ê³µê³ ì¼ ê°€ëŠ¥ì„±
                self.logger.warning(f"ğŸš¨ [IMAGE_HEAVY] ì´ë¯¸ì§€ ìœ„ì£¼ ì½˜í…ì¸  ê°ì§€ ({len(img_elements)}ê°œ): {iframe_url}")
                img_srcs = [img.get('src', '') for img in img_elements[:5]]  # ìƒìœ„ 5ê°œë§Œ
                self._log_suspicious_content(iframe_url, str(img_srcs), "ì´ë¯¸ì§€_ìœ„ì£¼")
                return
            
            # Canvas ìš”ì†Œ ê°ì§€ (ê·¸ë ¤ì§„ ì´ë¯¸ì§€ì¼ ê°€ëŠ¥ì„±)
            canvas_elements = soup.find_all('canvas')
            if canvas_elements:
                self.logger.warning(f"ğŸš¨ [CANVAS_DETECTED] Canvas ìš”ì†Œ ê°ì§€: {iframe_url}")
                self._log_suspicious_content(iframe_url, str(canvas_elements), "Canvas_ì½˜í…ì¸ ")
                return
            
            # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ê³  ë‹¤ë¥¸ ìš”ì†Œê°€ ë§ì€ ê²½ìš°
            user_content = soup.find('div', class_='user_content')
            if user_content:
                text_content = user_content.get_text(strip=True)
                all_elements = user_content.find_all(True)  # ëª¨ë“  HTML ìš”ì†Œ
                
                # í…ìŠ¤íŠ¸ ëŒ€ë¹„ HTML ìš”ì†Œê°€ ë§ìœ¼ë©´ êµ¬ì¡°í™”ëœ ë¹„í…ìŠ¤íŠ¸ ì½˜í…ì¸ ì¼ ê°€ëŠ¥ì„±
                if len(text_content) < 100 and len(all_elements) > 10:
                    self.logger.warning(f"ğŸš¨ [STRUCTURED_NON_TEXT] êµ¬ì¡°í™”ëœ ë¹„í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì˜ì‹¬: {iframe_url}")
                    element_tags = [elem.name for elem in all_elements[:10]]
                    self._log_suspicious_content(iframe_url, f"í…ìŠ¤íŠ¸:{len(text_content)}ê¸€ì, ìš”ì†Œ:{element_tags}", "êµ¬ì¡°í™”ëœ_ë¹„í…ìŠ¤íŠ¸")
                    
        except Exception as e:
            self.logger.debug(f"ë¹„í…ìŠ¤íŠ¸ ì½˜í…ì¸  ê°ì§€ ì˜¤ë¥˜: {e}")

    def _log_suspicious_content(self, iframe_url: str, content_sample: str, content_type: str):
        """ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì½˜í…ì¸ ë¥¼ ë³„ë„ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡"""
        try:
            import os
            from datetime import datetime
            
            # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
            log_dir = "logs/saramin_suspicious_content"
            os.makedirs(log_dir, exist_ok=True)
            
            # ë‚ ì§œë³„ ë¡œê·¸ íŒŒì¼
            today = datetime.now().strftime("%Y-%m-%d")
            log_file = os.path.join(log_dir, f"suspicious_content_{today}.log")
            
            # ë¡œê·¸ ì—”íŠ¸ë¦¬ ì‘ì„±
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            log_entry = f"""
=== {timestamp} ===
TYPE: {content_type}
URL: {iframe_url}
CONTENT_SAMPLE: {content_sample[:500]}...
{'=' * 50}
"""
            
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
                
            self.logger.debug(f"ğŸ“ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì½˜í…ì¸  ë¡œê·¸ ê¸°ë¡: {log_file}")
            
        except Exception as e:
            self.logger.debug(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì½˜í…ì¸  ë¡œê¹… ì˜¤ë¥˜: {e}")

    # ================== ê¸°ì¡´ ë©”ì„œë“œë“¤ (fallback ìš©ë„) ==================
    
    def _extract_job_title(self, metadata: dict, job_section: BeautifulSoup) -> str:
        if metadata.get('position_title'):
            return metadata.get('position_title')
        
        try:
            selectors = ['h1.tit_job', 'div.tit_area h1', 'h1', '.job_title h1']
            for selector in selectors:
                element = job_section.select_one(selector)
                if element:
                    return element.get_text(strip=True)
        except Exception:
            pass
        
        return ""
    
    def _extract_company_name(self, metadata: dict, job_section: BeautifulSoup) -> str:
        if metadata.get('company_name'):
            return metadata.get('company_name')
        
        try:
            selectors = ['.company_name', '.cp_area .company', '.tit_company']
            for selector in selectors:
                element = job_section.select_one(selector)
                if element:
                    return element.get_text(strip=True)
        except Exception:
            pass
        
        return ""
    
    def _extract_work_type(self, metadata: dict, job_section: BeautifulSoup) -> str:
        """ê·¼ë¬´í˜•íƒœ ì¶”ì¶œ - jv_summaryë¥¼ ìš°ì„ ìœ¼ë¡œ í•¨"""
        try:
            # 1. jv_summaryì—ì„œ ìš°ì„  ì¶”ì¶œ (ì´ë¯¸ mainì—ì„œ ì²˜ë¦¬ë¨)
            summary_section = job_section.find('div', class_='jv_summary')
            if summary_section:
                dl_elements = summary_section.find_all('dl')
                for dl in dl_elements:
                    dt = dl.find('dt')
                    dd = dl.find('dd')
                    if dt and dd and dt.get_text(strip=True) == 'ê·¼ë¬´í˜•íƒœ':
                        work_type = dd.get_text(strip=True)
                        self.logger.debug(f"âœ… ê·¼ë¬´í˜•íƒœ ì¶”ì¶œ: {work_type}")
                        return work_type
            
            # 2. fallback: ê¸°ì¡´ ë°©ì‹
            work_info_section = job_section.find('div', class_='job_info')
            if work_info_section:
                dt_elements = work_info_section.find_all('dt')
                dd_elements = work_info_section.find_all('dd')
                
                for dt, dd in zip(dt_elements, dd_elements):
                    if 'ê³ ìš©í˜•íƒœ' in dt.get_text() or 'ê·¼ë¬´í˜•íƒœ' in dt.get_text():
                        return dd.get_text(strip=True)
                        
        except Exception as e:
            self.logger.debug(f"ê·¼ë¬´í˜•íƒœ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
        return ""
    
    def _extract_location_info(self, metadata: dict, job_section: BeautifulSoup) -> dict:
        """ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ - jv_summaryë¥¼ ìš°ì„ ìœ¼ë¡œ í•¨"""
        location_text = ""
        
        try:
            # 1. jv_summaryì—ì„œ ìš°ì„  ì¶”ì¶œ
            summary_section = job_section.find('div', class_='jv_summary')
            if summary_section:
                dl_elements = summary_section.find_all('dl')
                for dl in dl_elements:
                    dt = dl.find('dt')
                    dd = dl.find('dd')
                    if dt and dd and dt.get_text(strip=True) == 'ê·¼ë¬´ì§€ì—­':
                        location_text = dd.get_text(strip=True)
                        self.logger.debug(f"âœ… ê·¼ë¬´ì§€ì—­ ì¶”ì¶œ: {location_text}")
                        break
            
            # 2. fallback: ë©”íƒ€ë°ì´í„°
            if not location_text:
                location_text = metadata.get('location', '')
            
            # 3. fallback: work_place í´ë˜ìŠ¤
            if not location_text:
                location_element = job_section.select_one('.work_place')
                if location_element:
                    location_text = location_element.get_text(strip=True)
                    
        except Exception as e:
            self.logger.debug(f"ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return self._normalize_location(location_text)
    
    def _extract_education(self, metadata: dict, job_section: BeautifulSoup) -> str:
        """í•™ë ¥ ì •ë³´ ì¶”ì¶œ - jv_summaryë¥¼ ìš°ì„ ìœ¼ë¡œ í•¨"""
        try:
            # 1. jv_summaryì—ì„œ ìš°ì„  ì¶”ì¶œ
            summary_section = job_section.find('div', class_='jv_summary')
            if summary_section:
                dl_elements = summary_section.find_all('dl')
                for dl in dl_elements:
                    dt = dl.find('dt')
                    dd = dl.find('dd')
                    if dt and dd and dt.get_text(strip=True) == 'í•™ë ¥':
                        education = dd.get_text(strip=True)
                        self.logger.debug(f"âœ… í•™ë ¥ ì¶”ì¶œ: {education}")
                        return education
            
            # 2. fallback: ë©”íƒ€ë°ì´í„°
            if metadata.get('education'):
                return metadata.get('education')
            
            # 3. fallback: qualification ì„¹ì…˜
            qualifications = job_section.find('div', class_='qualification')
            if qualifications:
                education_text = qualifications.get_text()
                return self._extract_education_from_text(education_text)
                
        except Exception as e:
            self.logger.debug(f"í•™ë ¥ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return ""
    
    def _extract_experience_info(self, metadata: dict, job_section: BeautifulSoup) -> dict:
        """ê²½ë ¥ ì •ë³´ ì¶”ì¶œ - jv_summaryë¥¼ ìš°ì„ ìœ¼ë¡œ í•¨"""
        try:
            # 1. jv_summaryì—ì„œ ìš°ì„  ì¶”ì¶œ
            summary_section = job_section.find('div', class_='jv_summary')
            if summary_section:
                dl_elements = summary_section.find_all('dl')
                for dl in dl_elements:
                    dt = dl.find('dt')
                    dd = dl.find('dd')
                    if dt and dd and dt.get_text(strip=True) == 'ê²½ë ¥':
                        experience_text = dd.get_text(strip=True)
                        parsed_exp = self._parse_experience(experience_text)
                        self.logger.debug(f"âœ… ê²½ë ¥ ì¶”ì¶œ: {experience_text} -> {parsed_exp}")
                        return parsed_exp
            
            # 2. fallback: ë©”íƒ€ë°ì´í„°
            if metadata.get('career'):
                return self._parse_experience(metadata.get('career'))
            
            # 3. fallback: qualification ì„¹ì…˜
            qualifications = job_section.find('div', class_='qualification')
            if qualifications:
                career_text = qualifications.get_text()
                return self._parse_experience(career_text)
                
        except Exception as e:
            self.logger.debug(f"ê²½ë ¥ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return {"raw_text": "", "min_years": 0, "max_years": 100}
    
    def _extract_position_info(self, metadata: dict, job_section: BeautifulSoup) -> dict:
        """í¬ì§€ì…˜ ì •ë³´ ì¶”ì¶œ ë° ì •ê·œí™”"""
        position_raw = {
            "raw_text": "",
            "raw_list": metadata.get('job_sectors', [])
        }
        
        # ì§ë¬´ ë¶„ì•¼ê°€ ìˆìœ¼ë©´ í™œìš©
        if position_raw["raw_list"]:
            position_raw["raw_text"] = ', '.join(position_raw["raw_list"])
        
        # soupì—ì„œ ì¶”ê°€ ì •ë³´
        try:
            job_category = job_section.find('div', class_='job_sector')
            if job_category:
                sectors = [span.get_text(strip=True) for span in job_category.find_all('span')]
                if sectors:
                    position_raw["raw_list"].extend(sectors)
                    position_raw["raw_text"] = ', '.join(position_raw["raw_list"])
        except Exception:
            pass
        
        # ì •ê·œí™” ì ìš©
        return normalize_position_data(
            position_raw["raw_text"], 
            position_raw["raw_list"]
        )
    
    def _extract_company_info(self, job_section: BeautifulSoup, company_name: str, metadata: dict) -> dict:
        """íšŒì‚¬ ì •ë³´ ì¶”ì¶œ - jv_company ì„¹ì…˜ì˜ info_areaì—ì„œ ì¶”ì¶œ"""
        company = {
            "name": company_name,
            "size": "",
            "description": "",
            "sales": "",
            "industry": ""
        }
        
        try:
            # jv_company ì„¹ì…˜ì˜ info_areaì—ì„œ ì¶”ì¶œ
            company_section = job_section.find('div', class_='jv_company')
            if company_section:
                info_area = company_section.find('div', class_='info_area')
                if info_area:
                    info_items = info_area.find_all('dl')
                    
                    for item in info_items:
                        dt = item.find('dt')
                        dd = item.find('dd')
                        
                        if dt and dd:
                            key = dt.get_text(strip=True)
                            value = dd.get_text(strip=True)
                            
                            if 'ì—…ì¢…' in key:
                                company['industry'] = value
                                self.logger.debug(f"âœ… íšŒì‚¬ ì—…ì¢…: {value}")
                            elif 'ê¸°ì—…í˜•íƒœ' in key:
                                company['size'] = value
                                self.logger.debug(f"âœ… íšŒì‚¬ ê·œëª¨/í˜•íƒœ: {value}")
                            elif 'ì‚¬ì›ìˆ˜' in key:
                                # ê¸°ì¡´ sizeê°€ ì—†ìœ¼ë©´ ì‚¬ì›ìˆ˜ë¡œ, ìˆìœ¼ë©´ ì¶”ê°€
                                if not company['size']:
                                    company['size'] = value
                                else:
                                    company['size'] += f", {value}"
                                self.logger.debug(f"âœ… íšŒì‚¬ ì‚¬ì›ìˆ˜: {value}")
                            elif 'ì„¤ë¦½ì¼' in key:
                                if company['description']:
                                    company['description'] += f", ì„¤ë¦½: {value}"
                                else:
                                    company['description'] = f"ì„¤ë¦½: {value}"
                                self.logger.debug(f"âœ… íšŒì‚¬ ì„¤ë¦½ì¼: {value}")
                            elif 'ë§¤ì¶œ' in key:
                                company['sales'] = value
                                self.logger.debug(f"âœ… íšŒì‚¬ ë§¤ì¶œ: {value}")
            
            # fallback: ê¸°ì¡´ cp_info ë°©ì‹
            if not any([company['size'], company['industry'], company['description']]):
                company_info = job_section.find('div', class_='cp_info')
                if company_info:
                    info_items = company_info.find_all('dl')
                    
                    for item in info_items:
                        dt = item.find('dt')
                        dd = item.find('dd')
                        
                        if dt and dd:
                            key = dt.get_text(strip=True)
                            value = dd.get_text(strip=True)
                            
                            if 'ì—…ì¢…' in key:
                                company['industry'] = value
                            elif 'ê·œëª¨' in key or 'ì‚¬ì›ìˆ˜' in key:
                                company['size'] = value
                            elif 'ë§¤ì¶œ' in key:
                                company['sales'] = value
                            elif 'ì„¤ë¦½' in key:
                                if company['description']:
                                    company['description'] += f", ì„¤ë¦½: {value}"
                                else:
                                    company['description'] = f"ì„¤ë¦½: {value}"
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´
            if metadata.get('company_type'):
                if company['description']:
                    company['description'] += f", {metadata['company_type']}"
                else:
                    company['description'] = metadata['company_type']
                    
        except Exception as e:
            self.logger.debug(f"íšŒì‚¬ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return company