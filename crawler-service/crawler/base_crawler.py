import requests
from bs4 import BeautifulSoup
import time
import logging
from datetime import datetime
from playwright.sync_api import sync_playwright
from urllib.parse import urlencode, urljoin, urlparse
import random
import re
import hashlib
from abc import ABC, abstractmethod
from dataclasses import dataclass
import os

from config.site_configs import SITE_CONFIGS, CrawlMethod, CRAWL_DAYS_BACK
from models.data_models import JobPostingModel
from utils.database import DatabaseManager

from utils.position_normalizer import normalize_position_data

@dataclass
class CrawlResult:
    site_name: str
    total_found: int
    new_saved: int
    duplicates: int
    errors: int
    execution_time: float
    logger: logging.Logger
    
    def print_summary(self):
        self.logger.info(f"   ğŸ¯ {self.site_name} í¬ë¡¤ë§ ì™„ë£Œ:")
        self.logger.info(f"   ğŸ“Š ì´ ë°œê²¬: {self.total_found}ê°œ")
        self.logger.info(f"   âœ… ì‹ ê·œ ì €ì¥: {self.new_saved}ê°œ")
        self.logger.info(f"   âšª ì¤‘ë³µ: {self.duplicates}ê°œ")
        self.logger.info(f"   âŒ ì—ëŸ¬: {self.errors}ê°œ")
        self.logger.info(f"   â±ï¸  ì†Œìš”ì‹œê°„: {self.execution_time:.1f}ì´ˆ")
        self.logger.info(f"================================================")


class JobCrawler(ABC):
    """
    ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    
    ê³µí†µ ê¸°ëŠ¥:
    - í¬ë¡¤ë§ ì‹¤í–‰ íë¦„ ê´€ë¦¬ (crawl_site)
    - ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ (Playwright, HTTP ì„¸ì…˜)
    - ë¡œê¹… ì„¤ì •
    - ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
    - ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    
    ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„í•´ì•¼ í•  ë©”ì„œë“œ:
    - _get_job_urls: ì‚¬ì´íŠ¸ë³„ URL ìˆ˜ì§‘ ë¡œì§
    - _crawl_job_detail: ì‚¬ì´íŠ¸ë³„ ìƒì„¸ í¬ë¡¤ë§ ë¡œì§
    """
    
    def __init__(self, site_name: str):
        self.db_manager = DatabaseManager(site_name=site_name)
        
        # HTTP ì„¸ì…˜ ì„¤ì • (requestsìš©)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Playwright ì¸ìŠ¤í„´ìŠ¤ (í•„ìš”ì‹œ ìƒì„±)
        self.playwright = None
        self.browser = None
    
        self.site_name = site_name
        
        # ë¡œê¹… ì„¤ì •
        current_time = datetime.now().strftime('%Y%m%d_%H%M')
        log_dir = f'logs/{current_time}'
        os.makedirs(log_dir, exist_ok=True)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'{log_dir}/crawler_{self.site_name}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
    
    def _init_playwright(self, config):
        """Playwright ì´ˆê¸°í™” (í•„ìš”ì‹œì—ë§Œ)"""
        if not self.playwright:
            self.playwright = sync_playwright().start()
            self.browser = self.playwright.chromium.launch(
                headless=config.playwright_options.get('headless', True)
            )
    
    def crawl_site(self, site_name: str, full_crawl: bool = False) -> CrawlResult:
        """
        ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰
        """
        start_time = time.time()
        config = SITE_CONFIGS[site_name]
        
        self.logger.info(f"================================================")
        self.logger.info(f"ğŸš€ {site_name} í¬ë¡¤ë§ ì‹œì‘ - {datetime.now()}")
        self.logger.info(f"ğŸŒ Base URL: {config.base_url}")
        
        #1: ê¸°ì¡´ URL ì¡°íšŒ (ì¤‘ë³µ ë°©ì§€)
        existing_urls = self.db_manager.get_existing_job_urls(site_name, CRAWL_DAYS_BACK) if not full_crawl else set()
        self.logger.info(f"ğŸ“š {site_name}: {len(existing_urls)}ê°œ ê¸°ì¡´ URL ë°œê²¬")
        
        #2: ì±„ìš©ê³µê³  ëª©ë¡ì—ì„œ URL ìˆ˜ì§‘ (ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        job_urls = self._get_job_urls(config, full_crawl)
        
        #3: ì‹ ê·œ URL í•„í„°ë§
        #new_urls = [url for url in job_urls if url not in existing_urls]
        new_urls = job_urls #ì‹œí—˜ìš©ìœ¼ë¡œ ì¤‘ë³µ ì œê±° ë¹„í™œì„±í™”
        
        self.logger.info(f"ğŸ†• {site_name}: {len(new_urls)}ê°œ ì‹ ê·œ URL (ì´ {len(job_urls)}ê°œ ì¤‘)")
        
        #4: ê°œë³„ í˜ì´ì§€ í¬ë¡¤ë§ (ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
        new_saved = 0
        duplicates = len(job_urls) - len(new_urls)
        errors = 0
        
        for i, url in enumerate(new_urls, 1):
            try:
                job_posting = self._crawl_job_detail(url, config)
                
                if job_posting:
                    # JobPostingModelì„ dictë¡œ ë³€í™˜í•´ì„œ ì €ì¥
                    doc_id = self.db_manager.save_job_posting(job_posting)
                    
                    if doc_id:
                        new_saved += 1
                        company_name = job_posting.company.get('name', 'Unknown')
                        self.logger.info(f"âœ… Saved: [{company_name}] {job_posting.job_title} - {url}")
                    else:
                        self.logger.info(f"âšª Duplicate: {url}")
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                if i % 5 == 0:
                    self.logger.info(f"ğŸ“ˆ {site_name}: {i}/{len(new_urls)} processed")
                
            except Exception as e:
                self.logger.error(f"âŒ Error processing {url}: {e}")
                errors += 1
        
        execution_time = time.time() - start_time
        
        return CrawlResult(
            site_name=site_name,
            total_found=len(job_urls),
            new_saved=new_saved,
            duplicates=duplicates,
            errors=errors,
            execution_time=execution_time, 
            logger=self.logger
        )
    
    @abstractmethod
    def _get_job_urls(self, config, full_crawl: bool = False) -> list:
        """
        ì±„ìš©ê³µê³  URL ìˆ˜ì§‘ (ì‚¬ì´íŠ¸ë³„ êµ¬í˜„ í•„ìš”)
        
        Args:
            config: ì‚¬ì´íŠ¸ ì„¤ì •
            full_crawl: ì „ì²´ í¬ë¡¤ë§ ì—¬ë¶€
            
        Returns:
            ì±„ìš©ê³µê³  URL ë¦¬ìŠ¤íŠ¸
        """
        pass
    
    @abstractmethod
    def _crawl_job_detail(self, url: str, config) -> JobPostingModel:
        """
        ê°œë³„ ì±„ìš©ê³µê³  í¬ë¡¤ë§ (ì‚¬ì´íŠ¸ë³„ êµ¬í˜„ í•„ìš”)
        
        Args:
            url: ì±„ìš©ê³µê³  URL
            config: ì‚¬ì´íŠ¸ ì„¤ì •
            
        Returns:
            JobPostingModel ê°ì²´
        """
        pass

    # ================== ê³µí†µ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ ==================
    
    def _get_common_headers(self, config, referer: str = None) -> dict:
        """ê³µí†µ í—¤ë” ìƒì„±"""
        headers = config.common_headers.copy() if config.common_headers else {}
        if referer:
            headers['Referer'] = referer
        return headers
    
    def _normalize_url(self, href: str, base_url: str) -> str:
        """URL ì •ê·œí™”"""
        if not href:
            return ""
        
        if href.startswith('http'):
            return href
        elif href.startswith('//'):
            return f"https:{href}"
        elif href.startswith('/'):
            return urljoin(base_url, href)
        else:
            return urljoin(base_url, f"/{href}")

    def _normalize_location(self, location_text: str) -> dict:
        """ìœ„ì¹˜ ì •ë³´ë¥¼ ì •ê·œí™”í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜"""
        if not location_text:
            return {"raw_text": "", "city": "", "district": ""}
        
        # ê¸°ë³¸ ì •ê·œí™” ë¡œì§
        location = {
            "raw_text": location_text,
            "city": "",
            "district": ""
        }
        
        try:
            # ê°„ë‹¨í•œ íŒŒì‹± (ì„œìš¸, ê²½ê¸°, ë¶€ì‚° ë“±)
            if "ì„œìš¸" in location_text:
                location["city"] = "ì„œìš¸"
                # êµ¬ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: ê°•ë‚¨êµ¬, ì„œì´ˆêµ¬ ë“±)
                import re
                district_match = re.search(r'([ê°€-í£]+êµ¬)', location_text)
                if district_match:
                    location["district"] = district_match.group(1)
            elif "ê²½ê¸°" in location_text:
                location["city"] = "ê²½ê¸°"
            elif "ë¶€ì‚°" in location_text:
                location["city"] = "ë¶€ì‚°"
            elif "ëŒ€êµ¬" in location_text:
                location["city"] = "ëŒ€êµ¬"
            elif "ì¸ì²œ" in location_text:
                location["city"] = "ì¸ì²œ"
            elif "ê´‘ì£¼" in location_text:
                location["city"] = "ê´‘ì£¼"
            elif "ëŒ€ì „" in location_text:
                location["city"] = "ëŒ€ì „"
            elif "ìš¸ì‚°" in location_text:
                location["city"] = "ìš¸ì‚°"
            
        except Exception:
            pass
        
        return location
    
    def _parse_experience(self, exp_text: str) -> dict:
        """ê²½ë ¥ ì •ë³´ íŒŒì‹± (ê³µí†µ ë¡œì§)"""
        experience = {"raw_text": exp_text, "min_years": 0, "max_years": 0}
        
        if not exp_text:
            return experience
            
        # ì‹ ì… ì²´í¬
        if any(keyword in exp_text for keyword in ["ì‹ ì…", "ê²½ë ¥ë¬´ê´€"]):
            experience["min_years"] = 0
            experience["max_years"] = 0
            return experience
        
        # ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r'(\d+)', exp_text)
        if numbers:
            numbers = [int(n) for n in numbers]
            
            if "ì´ìƒ" in exp_text:
                experience["min_years"] = numbers[0]
                experience["max_years"] = 99
            elif len(numbers) >= 2:
                experience["min_years"] = min(numbers)
                experience["max_years"] = max(numbers)
            elif len(numbers) == 1:
                experience["min_years"] = numbers[0]
                experience["max_years"] = numbers[0]
        
        return experience
    
    # def _extract_techs_from_text(self, text: str) -> list:
    #     """í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ ìŠ¤íƒ í‚¤ì›Œë“œ ì¶”ì¶œ (ê³µí†µ ë¡œì§)"""
    #     if not text:
    #         return []
        
    #     # í™•ì¥ëœ ê¸°ìˆ ìŠ¤íƒ í‚¤ì›Œë“œë“¤
    #     tech_keywords = [
    #         # ì–¸ì–´
    #         'JavaScript', 'TypeScript', 'Python', 'Java', 'C++', 'C#', 'Go', 'Rust', 'Swift', 'Kotlin',
    #         'PHP', 'Ruby', 'Scala', 'Dart', 'HTML', 'CSS',
            
    #         # í”„ë ˆì„ì›Œí¬/ë¼ì´ë¸ŒëŸ¬ë¦¬
    #         'Node.js', 'React', 'Vue.js', 'Vue', 'Angular', 'Nest.js', 'NestJS', 'Express', 'Express.js',
    #         'Spring', 'Spring Boot', 'Django', 'Flask', 'FastAPI', 'Next.js', 'NextJS', 'Nuxt.js',
    #         'jQuery', 'Bootstrap', 'Tailwind', 'Flutter', 'React Native',
            
    #         # ë°ì´í„°ë² ì´ìŠ¤
    #         'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Oracle', 'SQLite', 'MariaDB', 'Elasticsearch',
    #         'DynamoDB', 'Cassandra', 'InfluxDB',
            
    #         # í´ë¼ìš°ë“œ/ì¸í”„ë¼
    #         'AWS', 'Azure', 'GCP', 'Google Cloud', 'Docker', 'Kubernetes', 'Jenkins', 'GitLab CI',
    #         'GitHub Actions', 'Terraform', 'Ansible', 'Nginx', 'Apache',
            
    #         # ë©”ì‹œì§•/í
    #         'RabbitMQ', 'Kafka', 'Apache Kafka', 'SQS', 'Pub/Sub',
            
    #         # ëª¨ë‹ˆí„°ë§/ë¡œê¹…
    #         'Elastic Stack', 'ELK', 'Prometheus', 'Grafana', 'CloudWatch', 'DataDog',
            
    #         # API/í†µì‹ 
    #         'REST', 'RESTful', 'GraphQL', 'gRPC', 'WebSocket', 'Socket.io',
            
    #         # ë°©ë²•ë¡ /ë„êµ¬
    #         'Git', 'GitHub', 'GitLab', 'TDD', 'BDD', 'Agile', 'Scrum', 'CI/CD', 'DevOps',
    #         'Microservice', 'MSA', 'Serverless', 'Container'
    #     ]
        
    #     found_techs = []
    #     text_lower = text.lower()
        
    #     for tech in tech_keywords:
    #         # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰
    #         if tech.lower() in text_lower:
    #             # ì •í™•í•œ ë§¤ì¹­ì„ ìœ„í•´ ë‹¨ì–´ ê²½ê³„ ì²´í¬
    #             pattern = r'\b' + re.escape(tech.lower()) + r'\b'
    #             if re.search(pattern, text_lower):
    #                 found_techs.append(tech)
        
    #     return found_techs
    
    def _extract_techs_from_text(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ ìŠ¤íƒ í‚¤ì›Œë“œ ì¶”ì¶œ (ëŒ€í­ í™•ì¥ëœ 250ê°œ+ í‚¤ì›Œë“œ)"""
        if not text:
            return []
        
        tech_keywords = [
            # í”„ë¡œê·¸ë˜ë° ì–¸ì–´
            'JavaScript', 'TypeScript', 'Python', 'Java', 'C++', 'C#', 'Go', 'Rust', 'Swift', 'Kotlin',
            'PHP', 'Ruby', 'Scala', 'Dart', 'HTML', 'CSS', 'SQL', 'R', 'MATLAB', 'Objective-C',
            
            # ì›¹ í”„ë ˆì„ì›Œí¬/ë¼ì´ë¸ŒëŸ¬ë¦¬
            'Node.js', 'React', 'Vue.js', 'Vue', 'Angular', 'Nest.js', 'NestJS', 'Express', 'Express.js',
            'Spring', 'Spring Boot', 'Django', 'Flask', 'FastAPI', 'Next.js', 'NextJS', 'Nuxt.js',
            'jQuery', 'Bootstrap', 'Tailwind', 'Tailwind CSS', 'Material-UI', 'Ant Design',
            'Laravel', 'CodeIgniter', 'Symfony', 'CakePHP', 'Rails', 'Ruby on Rails',
            
            # ëª¨ë°”ì¼ ê°œë°œ
            'Flutter', 'React Native', 'Xamarin', 'Ionic', 'PhoneGap', 'Cordova',
            # iOS ì „ìš© (í…ŒìŠ¤íŠ¸ì—ì„œ ì„±ê³µí•œ í‚¤ì›Œë“œë“¤)
            'SwiftUI', 'UIKit', 'Combine', 'SnapKit', 'Swift Concurrency', 'RxSwift', 'Alamofire',
            'Core Data', 'CloudKit', 'WebKit', 'MapKit', 'AVFoundation', 'ARKit', 'Core ML',
            'Auto Layout', 'AutoLayout', 'Storyboard', 'XIB', 'MVVM', 'MVP', 'MVC', 'VIPER',
            'SPM', 'CocoaPods', 'Carthage', 'Xcode', 'Instruments', 'TestFlight', 'App Store Connect',
            'Xcode Cloud', 'Firebase', 'Realm', 'SQLite', 'UserNotifications', 'Push Notifications',
            # Android ì „ìš©
            'Android Studio', 'Gradle', 'Retrofit', 'OkHttp', 'Glide', 'Picasso', 'Room', 'LiveData',
            'ViewModel', 'Data Binding', 'View Binding', 'Jetpack Compose', 'Coroutines', 'RxJava',
            'Dagger', 'Hilt', 'Koin', 'Material Design', 'ConstraintLayout',
            
            # ë°ì´í„°ë² ì´ìŠ¤
            'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Oracle', 'SQLite', 'MariaDB', 'Elasticsearch',
            'DynamoDB', 'Cassandra', 'InfluxDB', 'Neo4j', 'CouchDB', 'Firebase Firestore',
            'Microsoft SQL Server', 'DB2', 'Supabase', 'PlanetScale',
            
            # í´ë¼ìš°ë“œ/ì¸í”„ë¼
            'AWS', 'Azure', 'GCP', 'Google Cloud', 'Digital Ocean', 'Heroku', 'Vercel', 'Netlify',
            'Docker', 'Kubernetes', 'Jenkins', 'GitLab CI', 'GitHub Actions', 'CircleCI', 'Travis CI',
            'Terraform', 'Ansible', 'Chef', 'Puppet', 'Vagrant', 'Nginx', 'Apache', 'HAProxy',
            'CloudFlare', 'CDN', 'Load Balancer',
            
            # ë©”ì‹œì§•/í
            'RabbitMQ', 'Kafka', 'Apache Kafka', 'SQS', 'Pub/Sub',
            
            # ëª¨ë‹ˆí„°ë§/ë¡œê¹…
            'Elastic Stack', 'ELK', 'Prometheus', 'Grafana', 'CloudWatch', 'DataDog',
            
            # API/í†µì‹ 
            'REST', 'RESTful', 'GraphQL', 'gRPC', 'WebSocket', 'Socket.io',
            
            # ê¸°íƒ€ ë„êµ¬/ê¸°ìˆ ë“¤
            'Git', 'GitHub', 'GitLab', 'TDD', 'BDD', 'Agile', 'Scrum', 'CI/CD', 'DevOps',
            'Microservice', 'MSA', 'Serverless', 'Container'
        ]
        
        found_techs = []
        text_lower = text.lower()
        
        for tech in tech_keywords:
            tech_lower = tech.lower()
            pattern = r'\b' + re.escape(tech_lower) + r'\b'
            
            if re.search(pattern, text_lower):
                original_match = re.search(pattern, text_lower)
                if original_match:
                    start, end = original_match.span()
                    actual_text = text[start:end]
                    found_techs.append(actual_text if actual_text else tech)
        
        return found_techs
    
    def _extract_education_from_text(self, requirements: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ í•™ë ¥ ì •ë³´ ì¶”ì¶œ (ê³µí†µ ë¡œì§)"""
        if not requirements:
            return None
        
        requirements_lower = requirements.lower()
        
        if 'í•™ë ¥ë¬´ê´€' in requirements_lower or 'ë¬´ê´€' in requirements_lower:
            return 'ë¬´ê´€'
        elif 'ê³ ë“±í•™êµ' in requirements_lower:
            return 'ê³ ì¡¸'
        elif 'í•™ì‚¬' in requirements_lower:
            return 'ëŒ€ì¡¸'
        elif 'ì„ì‚¬' in requirements_lower:
            return 'ì„ì‚¬'
        elif 'ë°•ì‚¬' in requirements_lower:
            return 'ë°•ì‚¬'
        
        return ''