import time
import re
from bs4 import BeautifulSoup
from datetime import datetime
from time import sleep
import random

from crawler.base_crawler import JobCrawler
from models.data_models import JobPostingModel
from utils.position_normalizer import normalize_position_data
from config.site_configs import SITE_CONFIGS, COMMON_BROWSER_HEADERS


class JobkoreaCrawler(JobCrawler):
    """
    ì¡ì½”ë¦¬ì•„ ì „ìš© í¬ë¡¤ëŸ¬
    
    íŠ¹ì§•:
    - Playwright ê¸°ë°˜ JavaScript ë Œë”ë§ í•„ìš”
    - ë³µì¡í•œ ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì‹œìŠ¤í…œ
    - í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
    - URLê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ìˆ˜ì§‘í•˜ì—¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ í™œìš©
    
    ì£¼ì˜ì‚¬í•­:
    - ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ì— ë§¤ìš° ì·¨ì•½í•¨
    - í´ë˜ìŠ¤ëª…ê³¼ ì…€ë ‰í„°ê°€ ìì£¼ ë³€ê²½ë¨
    - ì•ˆì •ì ì¸ í¬ë¡¤ë§ì„ ìœ„í•´ì„œëŠ” ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ í•„ìš”
    """
    
    def __init__(self, site_name: str):
        super().__init__(site_name)
        # URLë³„ ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œ (í•´ì²´ëœ êµ¬ì¡°ë¡œ ì €ì¥)
        self.url_metadata = {}
    
    def _get_job_urls(self, config, full_crawl: bool = False) -> list:
        """
        ì¡ì½”ë¦¬ì•„ URL ìˆ˜ì§‘ - ì¹´í…Œê³ ë¦¬ ë¶„í•  ì²˜ë¦¬ë¡œ ìµœëŒ€ 20ê°œ ì œí•œ ìš°íšŒ
        
        ë¬¸ì œ: ì¡ì½”ë¦¬ì•„ëŠ” ì¹´í…Œê³ ë¦¬ ì„ íƒì„ ìµœëŒ€ 20ê°œê¹Œì§€ë§Œ í—ˆìš©
        í•´ê²°: ì¹´í…Œê³ ë¦¬ë¥¼ 17ê°œì”© ë‚˜ëˆ„ì–´ì„œ ì—¬ëŸ¬ ë²ˆ ê²€ìƒ‰ ì‹¤í–‰
        """
        all_job_urls = []
        
        # ì¹´í…Œê³ ë¦¬ë¥¼ 17ê°œì”© ë¶„í•  (ìµœëŒ€ 20ê°œ ì œí•œ ê³ ë ¤í•˜ì—¬ ì—¬ìœ ë¶„ í™•ë³´)
        wanted_categories = [
            "ë°±ì—”ë“œê°œë°œì", "í”„ë¡ íŠ¸ì—”ë“œê°œë°œì", "ì›¹ê°œë°œì", "ì•±ê°œë°œì", 
            "ì‹œìŠ¤í…œì—”ì§€ë‹ˆì–´", "ë„¤íŠ¸ì›Œí¬ì—”ì§€ë‹ˆì–´", "DBA", "ë°ì´í„°ì—”ì§€ë‹ˆì–´", 
            "ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸", "ë³´ì•ˆì—”ì§€ë‹ˆì–´", "ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œì", 
            "ê²Œì„ê°œë°œì", "í•˜ë“œì›¨ì–´ê°œë°œì", "AI/MLì—”ì§€ë‹ˆì–´", "ë¸”ë¡ì²´ì¸ê°œë°œì",
            "í´ë¼ìš°ë“œì—”ì§€ë‹ˆì–´", "ì›¹í¼ë¸”ë¦¬ì…”", "ITì»¨ì„¤íŒ…", "QA", 
            "AI/MLì—°êµ¬ì›", "ë°ì´í„°ë¶„ì„ê°€", "ë°ì´í„°ë¼ë²¨ëŸ¬", "í”„ë¡¬í”„íŠ¸ì—”ì§€ë‹ˆì–´",
            "AIë³´ì•ˆì „ë¬¸ê°€", "MLOpsì—”ì§€ë‹ˆì–´", "AIì„œë¹„ìŠ¤ê°œë°œì"
        ]
        
        # 17ê°œì”© ì²­í¬ë¡œ ë¶„í• 
        chunk_size = 17
        category_chunks = [wanted_categories[i:i + chunk_size] 
                          for i in range(0, len(wanted_categories), chunk_size)]
        
        self.logger.info(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ {len(category_chunks)}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„í•  ì²˜ë¦¬")
        
        # Playwright ì´ˆê¸°í™”
        self._init_playwright(config)
        context = self.browser.new_context(
            viewport=config.playwright_options.get('viewport'),
            user_agent=config.playwright_options.get('user_agent')
        )
        
        try:
            for chunk_idx, category_chunk in enumerate(category_chunks, 1):
                self.logger.info(f"ğŸ¯ ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ {chunk_idx}/{len(category_chunks)} ì²˜ë¦¬ ì¤‘...")
                
                page = context.new_page()
                
                try:
                    # ê²€ìƒ‰ í˜ì´ì§€ ì ‘ì†
                    search_url = "https://www.jobkorea.co.kr/recruit/joblist?menucode=duty"
                    page.goto(search_url, wait_until='networkidle', timeout=60000)
                    page.wait_for_timeout(3000)
                    
                    # í˜„ì¬ ì²­í¬ì˜ ì¹´í…Œê³ ë¦¬ë“¤ ì„ íƒ
                    selected_categories = self._select_dev_categories_chunk(page, category_chunk)
                    
                    if not selected_categories:
                        self.logger.warning(f"âš ï¸ ê·¸ë£¹ {chunk_idx} ì¹´í…Œê³ ë¦¬ ì„ íƒ ì‹¤íŒ¨")
                        continue
                    
                    # ê²€ìƒ‰ ì‹¤í–‰
                    if not self._execute_search(page):
                        self.logger.warning(f"âš ï¸ ê·¸ë£¹ {chunk_idx} ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨")
                        continue
                    
                    # URL ìˆ˜ì§‘
                    chunk_urls = self._collect_urls_with_pagination(page, config, full_crawl)
                    all_job_urls.extend(chunk_urls)
                    
                    self.logger.info(f"âœ… ê·¸ë£¹ {chunk_idx}: {len(chunk_urls)}ê°œ URL ìˆ˜ì§‘")
                    
                finally:
                    page.close()
                    
        finally:
            context.close()
        
        # ì¤‘ë³µ ì œê±°
        unique_urls = list(set(all_job_urls))
        self.logger.debug(f"ğŸ¯ ì¡ì½”ë¦¬ì•„ ì „ì²´ URL ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_urls)}ê°œ ê³ ìœ  URL")
        
        return unique_urls

    def _select_dev_categories_chunk(self, page, category_chunk):
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì²­í¬ë§Œ ì„ íƒ - 20ê°œ ì œí•œ ìš°íšŒìš©"""
        selected_categories = []
        
        try:
            self.logger.info(f"ğŸ¯ ì¹´í…Œê³ ë¦¬ ì²­í¬ ì„ íƒ ì‹œì‘ ({len(category_chunk)}ê°œ)")
            
            # 1ë‹¨ê³„: ë©”ì¸ ì¹´í…Œê³ ë¦¬ í´ë¦­ (AIÂ·ê°œë°œÂ·ë°ì´í„°)
            main_category_selectors = [
                'label[for="duty_step1_10031"]',
                '#duty_step1_10031',
                'input[value="10031"]',
            ]
            
            main_category_clicked = False
            for selector in main_category_selectors:
                try:
                    element = page.query_selector(selector)
                    if element:
                        element.click(force=True)
                        page.wait_for_timeout(2000)
                        main_category_clicked = True
                        self.logger.info("âœ… AIÂ·ê°œë°œÂ·ë°ì´í„° ë©”ì¸ ì¹´í…Œê³ ë¦¬ í´ë¦­ ì„±ê³µ")
                        break
                except:
                    continue
            
            if not main_category_clicked:
                self.logger.warning("âš ï¸ AIÂ·ê°œë°œÂ·ë°ì´í„° ë©”ì¸ ì¹´í…Œê³ ë¦¬ í´ë¦­ ì‹¤íŒ¨")
                return []
            
            # 2ë‹¨ê³„: dev-sub ì„¹ì…˜ì—ì„œ í•´ë‹¹ ì²­í¬ì˜ ì¹´í…Œê³ ë¦¬ë§Œ ì„ íƒ
            dev_sub_section = page.query_selector('div.nano-content.dev-sub')
            if not dev_sub_section:
                self.logger.error("âŒ dev-sub ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            checkboxes = dev_sub_section.query_selector_all('input[type="checkbox"]')
            self.logger.info(f"ğŸ“‹ {len(checkboxes)}ê°œ ì²´í¬ë°•ìŠ¤ ë°œê²¬")
            
            # í˜„ì¬ ì²­í¬ì˜ ì¹´í…Œê³ ë¦¬ë§Œ ì„ íƒ
            selected_count = 0
            for checkbox in checkboxes:
                try:
                    data_name = checkbox.get_attribute('data-name')
                    
                    if data_name and data_name in category_chunk:
                        if not checkbox.is_checked():
                            page.evaluate('(element) => element.click()', checkbox)
                            page.wait_for_timeout(50)
                        
                        selected_categories.append(data_name)
                        selected_count += 1
                        self.logger.debug(f"âœ… ì„ íƒ: {data_name}")
                        
                except Exception:
                    continue
            
            self.logger.info(f"ğŸ¯ ì²­í¬ì—ì„œ ì´ {selected_count}ê°œ ì¹´í…Œê³ ë¦¬ ì„ íƒ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ì²­í¬ ì„ íƒ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return selected_categories

    def _execute_search(self, page):
        """ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­í•˜ì—¬ í•„í„° ì ìš©"""
        try:
            self.logger.debug("ğŸ” ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            
            # ë‹¤ì–‘í•œ ê²€ìƒ‰ ë²„íŠ¼ ì…€ë ‰í„° ì‹œë„
            search_selectors = [
                'button:has-text("ì„ íƒëœ ì¡°ê±´ ê²€ìƒ‰í•˜ê¸°")',
                'input[value*="ì„ íƒëœ ì¡°ê±´ ê²€ìƒ‰í•˜ê¸°"]',
                'button:has-text("ê²€ìƒ‰í•˜ê¸°")',
                '.btn-search'
            ]
            
            for selector in search_selectors:
                try:
                    button = page.query_selector(selector)
                    if button and button.is_visible():
                        button.click()
                        page.wait_for_timeout(5000)  # í•„í„°ë§ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°
                        self.logger.info("âœ… ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                        return True
                except:
                    continue
            
            self.logger.warning("âš ï¸ ê²€ìƒ‰ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _collect_urls_with_pagination(self, page, config, full_crawl):
        """í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ URL ìˆ˜ì§‘"""
        job_urls = []
        
        try:
            self.logger.info("ğŸ” í•„í„°ë§ëœ ê²°ê³¼ì—ì„œ URL ìˆ˜ì§‘ ì‹œì‘")
            
            max_pages = 10 if full_crawl else config.max_pages  # í…ŒìŠ¤íŠ¸ìš© ì œí•œ
            consecutive_empty = 0
            
            for page_num in range(1, max_pages + 1):
                try:
                    self.logger.info(f"ğŸ“„ ì¡ì½”ë¦¬ì•„ page {page_num} ì²˜ë¦¬ ì¤‘...")
                    
                    # í˜„ì¬ í˜ì´ì§€ì—ì„œ URL ì¶”ì¶œ + metadata ì €ì¥
                    page_urls = self._extract_urls_from_current_page(page)
                    
                    if not page_urls:
                        consecutive_empty += 1
                        self.logger.info(f"ğŸ“„ ë¹ˆ í˜ì´ì§€ ({consecutive_empty}/3)")
                        
                        if consecutive_empty >= 3:
                            self.logger.info("ğŸ›‘ ì—°ì† ë¹ˆ í˜ì´ì§€ë¡œ ì¸í•œ ìˆ˜ì§‘ ì¢…ë£Œ")
                            break
                    else:
                        consecutive_empty = 0
                        job_urls.extend(page_urls)
                        self.logger.info(f"ğŸ“„ page {page_num}: {len(page_urls)}ê°œ URL ìˆ˜ì§‘")
                    
                    # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                    if page_num < max_pages:
                        if not self._go_to_next_page(page, page_num + 1):
                            self.logger.info("ğŸ›‘ ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
                            break
                    
                except Exception as e:
                    self.logger.error(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    consecutive_empty += 1
                    if consecutive_empty >= 3:
                        break
            
        except Exception as e:
            self.logger.error(f"âŒ URL ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return job_urls

    def _extract_urls_from_current_page(self, page):
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ì±„ìš©ê³µê³  URLë“¤ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ì¶”ì¶œ"""
        job_urls = []
        
        try:
            # ì¡ì½”ë¦¬ì•„ ì±„ìš©ê³µê³  ë§í¬ íŒ¨í„´ - ì „ì²´ tr ìš”ì†Œ ì„ íƒ
            job_rows = page.query_selector_all('#dev-gi-list tr.devloopArea')
            self.logger.debug(f"ğŸ” ì¼ë°˜ ì±„ìš©ê³µê³  ì˜ì—­ì—ì„œ {len(job_rows)}ê°œ ê³µê³  ë°œê²¬")
            
            for row in job_rows:
                try:
                    # URL ì¶”ì¶œ
                    link = row.query_selector('td.tplTit strong a[href*="/Recruit/GI_Read/"]')
                    if not link:
                        continue
                    
                    # íšŒì‚¬ ì •ë³´ URL ì¶”ì¶œ
                    company_link = row.query_selector('td.tplCo a[href*="/Recruit/Co_Read/"]')
                    company_info_url = ''
                    if company_link:
                        company_info_url = company_link.get_attribute('href')
                        company_info_url = f"https://www.jobkorea.co.kr{company_info_url}"
                        
                    href = link.get_attribute('href')
                    if href:
                        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if href.startswith('/'):
                            full_url = f"https://www.jobkorea.co.kr{href}"
                        else:
                            full_url = href
                        
                        job_urls.append(full_url)
                        
                        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° í•´ì²´í•´ì„œ ì €ì¥
                        metadata = self._extract_metadata_from_row(row, company_info_url)
                        self.url_metadata[full_url] = metadata
                        
                        # ë¡œê¹…
                        company = metadata.get('company_name', 'Unknown')
                        title = metadata.get('title', 'Unknown')
                        self.logger.info(f"âœ… [FOUND] ì±„ìš©ê³µê³  | íšŒì‚¬: {company} | {title} | URL: {full_url}")
                        
                except Exception:
                    continue
            
        except Exception as e:
            self.logger.error(f"âŒ URL ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return job_urls

    def _extract_metadata_from_row(self, row, company_info_url: str) -> dict:
        """ëª©ë¡ í˜ì´ì§€ì˜ ê° rowì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (í•´ì²´ëœ êµ¬ì¡°)"""
        metadata = {
            'title': '',
            'company_name': '',
            'company_info_url': company_info_url,
            'position_description': '',
            'position_list': [],
            'location': '',
            'experience_raw': '',
            'experience_min_years': 0,
            'experience_max_years': 100,
            'education': '',
            'work_type': '',
            'position_level': ''
        }
        
        try:
            # ì œëª© ì¶”ì¶œ
            title_element = row.query_selector('td.tplTit strong a')
            if title_element:
                metadata['title'] = title_element.get_attribute('title')
            
            # íšŒì‚¬ëª… ì¶”ì¶œ
            company_element = row.query_selector('td.tplCo a')
            if company_element:
                metadata['company_name'] = company_element.text_content().strip()
            
            # ê¸°íƒ€ ì •ë³´ ì¶”ì¶œ (etc í´ë˜ìŠ¤)
            etc_elements = row.query_selector_all('td.tplTit p.etc span.cell')
            fields = ['experience_raw', 'education', 'location', 'work_type', 'position_level']
            values = [e.text_content().strip() for e in etc_elements]
            
            for i, field in enumerate(fields):
                if i < len(values):
                    if field == 'experience_raw':
                        metadata[field] = values[i]
                        # ê²½ë ¥ ì •ë³´ íŒŒì‹±í•˜ì—¬ min/maxë„ ì €ì¥
                        exp_parsed = self._parse_experience(values[i])
                        metadata['experience_min_years'] = exp_parsed['min_years']
                        metadata['experience_max_years'] = exp_parsed['max_years']
                    else:
                        metadata[field] = values[i]
                
            # í¬ì§€ì…˜ ì„¤ëª… ì¶”ì¶œ (.dsc í´ë˜ìŠ¤ - í•µì‹¬!)
            dsc_element = row.query_selector('td.tplTit p.dsc')
            if dsc_element:
                dsc_text = dsc_element.text_content().strip()
                metadata['position_description'] = dsc_text
                # ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                position_list = [pos.strip() for pos in dsc_text.split(',') if pos.strip()]
                metadata['position_list'] = position_list
                
        except Exception as e:
            self.logger.debug(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return metadata

    def _go_to_next_page(self, page, next_page_num):
        """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            # ì•µì»¤ ê¸°ë°˜ í˜ì´ì§€ ì´ë™ (#anchorGICnt_N)
            next_anchor = f"#anchorGICnt_{next_page_num}"
            current_url = page.url.split('#')[0]
            next_url = f"{current_url}{next_anchor}"
            
            self.logger.debug(f"ğŸ”— ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™: page {next_page_num}")
            page.goto(next_url, wait_until='networkidle', timeout=30000)
            page.wait_for_timeout(3000)
            sleep(random.uniform(1, 2))
            
            return True
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ í˜ì´ì§€ {next_page_num} ì´ë™ ì‹¤íŒ¨: {e}")
            return False

    def _crawl_job_detail(self, url: str, config) -> JobPostingModel:
        """
        ì¡ì½”ë¦¬ì•„ ê°œë³„ ì±„ìš©ê³µê³  í¬ë¡¤ë§ - BeautifulSoup ê¸°ë°˜ HTML íŒŒì‹±
        """
        try:
            # URLì—ì„œ ì±„ìš©ê³µê³  ID ì¶”ì¶œ
            job_id_match = re.search(r'/GI_Read/(\d+)', url)
            if not job_id_match:
                self.logger.error(f"âŒ ì¡ì½”ë¦¬ì•„ URLì—ì„œ job_id ì¶”ì¶œ ì‹¤íŒ¨: {url}")
                return None
            
            job_id = job_id_match.group(1)
            self.logger.info(f"ğŸ” ì¡ì½”ë¦¬ì•„ ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘: job_id={job_id}")
            
            # HTTP ìš”ì²­ìœ¼ë¡œ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            }
            sleep(random.uniform(1, 2))
            response = self.session.get(url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                self.logger.error(f"âŒ ì¡ì½”ë¦¬ì•„ HTTP ì˜¤ë¥˜: {response.status_code}")
                return None
            
            # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # JobPostingModel ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ì¶”ì¶œ
            job_posting = self._parse_jobkorea_detail_to_jobposting(soup, url, job_id)
            
            sleep(random.uniform(1, 2))
            return job_posting
            
        except Exception as e:
            self.logger.error(f"âŒ ì¡ì½”ë¦¬ì•„ í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
            return None

    def _parse_jobkorea_detail_to_jobposting(self, soup: BeautifulSoup, url: str, job_id: str) -> JobPostingModel:
        """
        ì¡ì½”ë¦¬ì•„ ìƒì„¸ í˜ì´ì§€ ì¢…í•© íŒŒì‹± - wanted ìˆœì„œì— ë§ì¶° ì •ë¦¬
        """
        
        # ================== ê¸°ë³¸ ì‹ë³„ ì •ë³´ ==================
        year = datetime.now().year
        full_job_id = f"jobkorea_{year}_{job_id}"
        platform = "jobkorea"
        job_url = url
        
        # URL ë©”íƒ€ë°ì´í„°ì—ì„œ ê¸°ë³¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        metadata = self.url_metadata.get(url, {})
        
        # ================== í”Œë«í¼ ë° íšŒì‚¬ ì •ë³´ ==================
        job_title = self._extract_job_title(metadata, soup)
        company_name = self._extract_company_name(metadata, soup)
        company = self._extract_company_info(soup, company_name, metadata.get('company_info_url', ''))
        
        # ================== ê³µê³  ê¸°ë³¸ ì •ë³´ ==================
        work_type = self._extract_work_type(metadata, soup)
        location = self._extract_location_info(metadata, soup)
        
        # ================== ìê²© ìš”ê±´ ==================
        education = self._extract_education(metadata, soup)
        experience = self._extract_experience_info(metadata, soup)
        
        # ================== í¬ì§€ì…˜ ì •ë³´ ==================
        position = self._extract_position_info(metadata, soup)
        
        # ================== ê¸°ìˆ  ìŠ¤íƒ ì •ë³´ ==================
        tech_stack = self._extract_tech_stack(soup)
        
        # ================== ìš°ëŒ€ ê²½í—˜/ì‚¬í•­ ==================
        preferred_experience = self._extract_preferred_experience(soup)
        
        # JobPostingModel ìƒì„±
        job_posting = JobPostingModel.create(
            job_id=full_job_id,
            job_url=job_url,
            platform=platform,
            job_title=job_title,
            company={
                "name": company_name,
                "size": company.get('size', ''),
                "description": company.get('description', ''),
                "sales": company.get('sales', ''),
                "industry": company.get('industry', '')
            },
            location=location,
            position=position,
            tech_stack=tech_stack,
            experience=experience,
            education=education,
            work_type=work_type,
            preferred_experience=preferred_experience
        )
        
        return job_posting

    # ================== ë°ì´í„° ì¶”ì¶œ ë©”ì„œë“œë“¤ ==================
    
    def _extract_job_title(self, metadata: dict, soup: BeautifulSoup) -> str:
        """ì§ë¬´ ì œëª© ì¶”ì¶œ"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ìš°ì„  ì¶”ì¶œ
        if metadata.get('title'):
            return metadata.get('title')
        
        # soupì—ì„œ ì¶”ì¶œ (fallback)
        try:
            title_element = soup.select_one('div.sumTit h3.hd_3')
            if title_element:
                title_text = title_element.get_text(strip=True)
                lines = title_text.split('\n')
                for line in lines:
                    line = line.strip()
                    if line and not any(x in line for x in ['ê´€ì‹¬ê¸°ì—…', 'ê¸°ì—…ì¸ì¦']):
                        import html
                        return html.unescape(line)
        except Exception:
            pass
        
        return ""
    
    def _extract_company_name(self, metadata: dict, soup: BeautifulSoup) -> str:
        """íšŒì‚¬ëª… ì¶”ì¶œ"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ìš°ì„  ì¶”ì¶œ
        if metadata.get('company_name'):
            return metadata.get('company_name')
        
        # soupì—ì„œ ì¶”ì¶œ (fallback)
        try:
            selectors = [
                'span.coName',
                'div.view-subtitle.dev-wrap-subtitle a.subtitle-corp'
            ]
            
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    return element.get_text(strip=True)
        except Exception:
            pass
        
        return ""
    
    def _extract_work_type(self, metadata: dict, soup: BeautifulSoup) -> str:
        """ê·¼ë¬´í˜•íƒœ ì¶”ì¶œ"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ìš°ì„  ì¶”ì¶œ
        if metadata.get('work_type'):
            return metadata.get('work_type')
        
        # soupì—ì„œ ì¶”ì¶œ (fallback)
        return self._extract_from_work_conditions_section(soup, 'ê³ ìš©í˜•íƒœ')
    
    def _extract_location_info(self, metadata: dict, soup: BeautifulSoup) -> dict:
        """ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ"""
        location_text = metadata.get('location', '')
        
        # soupì—ì„œ ì¶”ê°€ ì •ë³´ í™•ì¸
        if not location_text:
            location_text = self._extract_from_work_conditions_section(soup, 'ì§€ì—­')
        
        return self._normalize_location(location_text)
    
    def _extract_education(self, metadata: dict, soup: BeautifulSoup) -> str:
        """í•™ë ¥ ì •ë³´ ì¶”ì¶œ"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ìš°ì„  ì¶”ì¶œ
        if metadata.get('education'):
            return metadata.get('education')
        
        # soupì—ì„œ ì¶”ì¶œ (fallback)
        return self._extract_from_qualifications_section(soup, 'í•™ë ¥')
    
    def _extract_experience_info(self, metadata: dict, soup: BeautifulSoup) -> dict:
        """ê²½ë ¥ ì •ë³´ ì¶”ì¶œ"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ìš°ì„  ì¶”ì¶œ
        if metadata.get('experience_raw'):
            return {
                "raw_text": metadata.get('experience_raw'),
                "min_years": metadata.get('experience_min_years', 0),
                "max_years": metadata.get('experience_max_years', 100)
            }
        
        # soupì—ì„œ ì¶”ì¶œ (fallback)
        exp_text = self._extract_from_qualifications_section(soup, 'ê²½ë ¥')
        return self._parse_experience(exp_text)
    
    def _extract_position_info(self, metadata: dict, soup: BeautifulSoup) -> dict:
        """í¬ì§€ì…˜ ì •ë³´ ì¶”ì¶œ - pageviewObj.dimension44ì—ì„œ íŒŒì‹±"""
        try:
            # JavaScriptì—ì„œ pageviewObj.dimension44 ê°’ ì¶”ì¶œ
            script_tags = soup.find_all('script')
            
            for script in script_tags:
                if script.string and 'pageviewObj.dimension44' in script.string:
                    # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ dimension44 ê°’ ì¶”ì¶œ
                    pattern = r'pageviewObj\.dimension44\s*=\s*[\'"]([^\'"]*)[\'"]'
                    match = re.search(pattern, script.string)
                    
                    if match:
                        positions_string = match.group(1)
                        position_list = [pos.strip() for pos in positions_string.split('|') if pos.strip()]
                        
                        position_raw = {
                            "raw_text": positions_string.replace('|', ', '),
                            "raw_list": position_list
                        }
                        
                        # ì •ê·œí™” ì ìš©
                        return normalize_position_data(
                            position_raw["raw_text"], 
                            position_raw["raw_list"]
                        )
            
            # fallback: ê¸°ì¡´ metadata ë°©ì‹
            position_raw = {
                "raw_text": metadata.get('position_description', ''),
                "raw_list": metadata.get('position_list', [])
            }
            
            return normalize_position_data(
                position_raw["raw_text"], 
                position_raw["raw_list"]
            )
            
        except Exception as e:
            self.logger.debug(f"í¬ì§€ì…˜ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {"raw_text": "", "raw_list": [], "normalized": []}

    def _extract_tech_stack(self, soup: BeautifulSoup) -> dict:
        """ê¸°ìˆ  ìŠ¤íƒ ì •ë³´ ì¶”ì¶œ"""
        tech_stack = {"raw_text": "", "raw_list": []}
        
        try:
            # 1. ì˜›ë‚  ìŠ¤íƒ€ì¼ ë°•ìŠ¤ì—ì„œ ì¶”ì¶œ
            qualifications_section = soup.select_one('div.tbRow.clear div.tbCol dl.tbList')
            if qualifications_section:
                tech_text = self._extract_from_dt_dd_section(qualifications_section, 'ìŠ¤í‚¬')
                if tech_text:
                    tech_stack['raw_text'] = tech_text
                    tech_stack['raw_list'] = [tech.strip() for tech in tech_text.split(',') if tech.strip()]
                    return tech_stack
                    
            # 2. ì„¸ë ¨ëœ ìŠ¤íƒ€ì¼ ìŠ¤í‚¬ íƒœê·¸ì—ì„œ ì¶”ì¶œ
            tech_stack_section = soup.select_one('ul.view-content-detail-skill')
            if tech_stack_section:
                tech_items = tech_stack_section.find_all('li')
                tech_stack['raw_list'] = [item.get_text(strip=True) for item in tech_items]
                tech_stack['raw_text'] = ', '.join(tech_stack['raw_list'])
                        
        except Exception as e:
            self.logger.debug(f"ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return tech_stack

    def _extract_preferred_experience(self, soup: BeautifulSoup) -> dict:
        """ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ"""
        preferred = {"raw_text": "", "raw_list": []}
        
        try:
            preferred_section = soup.select_one('dl.tbAdd.tbPref')
            if preferred_section:
                dd_element = preferred_section.select_one('dd')
                if dd_element:
                    preferred_text = dd_element.get_text(strip=True)
                    preferred['raw_text'] = preferred_text
                    preferred_list = [item.strip() for item in preferred_text.split(',') if item.strip()]
                    preferred['raw_list'] = preferred_list
                    
        except Exception as e:
            self.logger.debug(f"ìš°ëŒ€ì‚¬í•­ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return preferred

    def _extract_company_info(self, soup: BeautifulSoup, company_name: str, company_info_url: str) -> dict:
        """íšŒì‚¬ ì •ë³´ ì¶”ì¶œ"""
        company = {
            "name": company_name,
            "size": "", #ëŒ€ê¸°ì—…/ì¤‘ê²¬/ì¤‘ì†Œ
            "description": "",
            "sales": "",
            "industry": ""
        }
        
        try:
            # 1. í˜„ì¬ í˜ì´ì§€ì˜ ê¸°ì—…ì •ë³´ í…Œì´ë¸”ì—ì„œ ì¶”ì¶œ
            self._extract_from_legacy_section(soup, company)
            
            # 2. í•„ìˆ˜ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ í˜ì´ì§€ì—ì„œ ë³´ì™„
            if not self._has_essential_company_info(company) and company_info_url:
                self._extract_from_company_page(company_info_url, company)
                
            # 3. ì•„ ì—¬ê¸°ì„œë„ ì•ˆëœë‹¤? íšŒì‚¬ ì •ë³´ í˜ì´ì§€ê°€ https://www.jobkorea.co.kr/company/1810241 ë“¤ì–´ê°€ë³´ë©´ ê¸°ì—…ì •ë³´ë¡œ í•œë²ˆ ë” ë“¤ì–´ê°€ì•¼í•˜ëŠ” êµ¬ì¡° ì´ê²½ìš°ì—ëŠ” company_info_urlëì—ë‹¤ê°€ "?tabType=l"ì„ ì¶”ê°€í•œë‹¤
            if not self._has_essential_company_info(company) and company_info_url:
                self._extract_from_company_page(company_info_url + "?tabType=l", company)
            return company
            
        except Exception as e:
            self.logger.debug(f"íšŒì‚¬ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return company

    # ================== í—¬í¼ ë©”ì„œë“œë“¤ ==================
    
    def _extract_from_qualifications_section(self, soup: BeautifulSoup, field_name: str) -> str:
        """ì§€ì›ìê²© ì„¹ì…˜ì—ì„œ íŠ¹ì • í•„ë“œ ì¶”ì¶œ"""
        try:
            qualifications_section = soup.select_one('div.tbRow.clear div.tbCol dl.tbList')
            if qualifications_section:
                return self._extract_from_dt_dd_section(qualifications_section, field_name)
        except Exception:
            pass
        return ""
    
    def _extract_from_work_conditions_section(self, soup: BeautifulSoup, field_name: str) -> str:
        """ê·¼ë¬´ì¡°ê±´ ì„¹ì…˜ì—ì„œ íŠ¹ì • í•„ë“œ ì¶”ì¶œ"""
        try:
            work_sections = soup.select('div.tbRow.clear div.tbCol')
            if len(work_sections) > 1:
                work_section = work_sections[1]
                work_list = work_section.select_one('dl.tbList')
                if work_list:
                    return self._extract_from_dt_dd_section(work_list, field_name)
        except Exception:
            pass
        return ""
    
    def _extract_from_dt_dd_section(self, section, field_name: str) -> str:
        """dt/dd êµ¬ì¡°ì—ì„œ íŠ¹ì • í•„ë“œ ì¶”ì¶œ"""
        try:
            dt_elements = section.find_all('dt')
            dd_elements = section.find_all('dd')
            
            for dt, dd in zip(dt_elements, dd_elements):
                if dt.get_text(strip=True) == field_name:
                    return dd.get_text(strip=True)
        except Exception:
            pass
        return ""
    
    def _extract_from_legacy_section(self, soup: BeautifulSoup, company: dict):
        """ê¸°ì—…ì •ë³´ í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            # # í˜„ì¬ í˜ì´ì§€ì˜ ê¸°ì—…ì •ë³´ í…Œì´ë¸”
            # company_info_section = soup.select_one('table.table-basic-infomation-primary')
            # if company_info_section:
            #     self._extract_from_table(company_info_section, company)
            #     return
                
            # ì˜›ë‚  ìŠ¤íƒ€ì¼ dt/dd êµ¬ì¡°
            legacy_section = soup.select_one('div.tbCol.coInfo dl.tbList')
            if legacy_section:
                field_mapping = {
                    'ì‚°ì—…': 'industry',
                    'ì‚¬ì›ìˆ˜': 'size', 
                    'ë§¤ì¶œì•¡': 'sales',
                    'ê¸°ì—…í˜•íƒœ': 'description',
                    'ì„¤ë¦½': 'description'
                }
                #self.logger.debug(f"ğŸ” ê¸°ì—…ì •ë³´ í…Œì´ë¸” ì¶”ì¶œ: {legacy_section}")
                
                for field_ko, field_en in field_mapping.items():
                    value = self._extract_from_dt_dd_section(legacy_section, field_ko)
                    if value:
                        if field_en == 'description' and company[field_en]:
                            company[field_en] += f", {value}"
                        else:
                            company[field_en] = value

        except Exception:
            pass
    
    def _has_essential_company_info(self, company: dict) -> bool:
        """í•„ìˆ˜ ì •ë³´ í™•ì¸"""
        return bool(company['size'] or company['sales'] or company['industry'])
    
    def _extract_from_company_page(self, company_info_url: str, company: dict):
        """íšŒì‚¬ ì •ë³´ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ"""
        try:
            headers = COMMON_BROWSER_HEADERS
            sleep(random.uniform(1, 2))
            response = self.session.get(company_info_url, headers=headers, timeout=30)
            self.logger.debug(f"ğŸ” íšŒì‚¬ í˜ì´ì§€ ìš”ì²­: {company_info_url}")
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # íšŒì‚¬ëª… ì¶”ì¶œ (ê¸°ì¡´ì— ì—†ì„ ê²½ìš°)
                if not company.get('name'):
                    company_name = self._extract_company_name_from_jobkorea(soup)
                    if company_name:
                        company['name'] = company_name
                        self.logger.debug(f"âœ… íšŒì‚¬ëª… ë°œê²¬: {company_name}")
                
                # ë©”ì¸ ì •ë³´ í…Œì´ë¸” ì°¾ê¸°
                company_info_section = soup.select_one('table.table-basic-infomation-primary')
                
                if company_info_section:
                    self.logger.debug("âœ… íšŒì‚¬ í˜ì´ì§€ì—ì„œ í…Œì´ë¸” ë°œê²¬")
                    self._extract_from_table(company_info_section, company)
                else:
                    # ë‹¤ë¥¸ í…Œì´ë¸” ì„ íƒì ì‹œë„
                    alternative_selectors = [
                        '.basic-infomation-container table',
                        '.company-infomation-container table'
                    ]
                    for selector in alternative_selectors:
                        table = soup.select_one(selector)
                        if table:
                            self.logger.debug(f"âœ… ëŒ€ì²´ í…Œì´ë¸” ë°œê²¬: {selector}")
                            self._extract_from_table(table, company)
                            break
                
                # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                self._extract_additional_jobkorea_info(soup, company)
                        
        except Exception as e:
            self.logger.debug(f"íšŒì‚¬ í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}")

    def _extract_company_name_from_jobkorea(self, soup):
        """ì¡ì½”ë¦¬ì•„ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ"""
        name_selectors = [
            '.company-header-branding-body .name',
            '.name',
            'title'
        ]
        
        for selector in name_selectors:
            element = soup.select_one(selector)
            if element:
                name = element.get_text(strip=True)
                if name and 'ì¡ì½”ë¦¬ì•„' not in name:
                    # ì œëª©ì—ì„œ íšŒì‚¬ëª…ë§Œ ì¶”ì¶œ
                    if selector == 'title':
                        parts = name.split(' ')
                        if len(parts) > 0:
                            name = parts[0]  # ì²« ë²ˆì§¸ ë¶€ë¶„ì´ íšŒì‚¬ëª…
                    return name
        return None

    def _extract_additional_jobkorea_info(self, soup, company):
        """ì¡ì½”ë¦¬ì•„ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì—…ì¢… ì •ë³´ (í—¤ë”ì—ì„œ)
            if not company.get('industry'):
                summary_item = soup.select_one('.summary-item')
                if summary_item:
                    industry = summary_item.get_text(strip=True)
                    if industry:
                        company['industry'] = industry
                        self.logger.debug(f"âœ… í—¤ë”ì—ì„œ ì—…ì¢… ë°œê²¬: {industry}")
            
            # ê¸°ì—…ì†Œê°œ
            if not company.get('description_long'):
                intro_element = soup.select_one('.introduce-body')
                if intro_element:
                    intro_text = intro_element.get_text(strip=True)
                    if intro_text and len(intro_text) > 10:
                        company['description_long'] = intro_text
                        self.logger.info(f"âœ… ê¸°ì—…ì†Œê°œ ë°œê²¬: {intro_text[:100]}...")
                        
        except Exception as e:
            self.logger.debug(f"ì¶”ê°€ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

    def _extract_from_table(self, table_element, company: dict):
        """í…Œì´ë¸” êµ¬ì¡°ì—ì„œ íšŒì‚¬ ì •ë³´ ì¶”ì¶œ (ì¡ì½”ë¦¬ì•„ ìµœì í™”)"""
        try:
            fields = table_element.select('tr.field')
            if not fields:
                fields = table_element.find_all('tr', class_='field')
            
            for field in fields:
                labels = field.find_all('th', class_='field-label')
                values = field.find_all('td', class_='field-value')
                
                for label, value in zip(labels, values):
                    label_text = label.get_text(strip=True)
                    
                    # ê°’ ì¶”ì¶œ - ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„
                    value_text = None
                    
                    # íŒ¨í„´ 1: div.value
                    value_element = value.select_one('div.value')
                    if value_element:
                        value_text = value_element.get_text(strip=True)
                    
                    # íŒ¨í„´ 2: div.values div.value  
                    if not value_text:
                        value_element = value.select_one('div.values div.value')
                        if value_element:
                            value_text = value_element.get_text(strip=True)
                    
                    # íŒ¨í„´ 3: ë§í¬ê°€ ìˆëŠ” ê²½ìš° (í™ˆí˜ì´ì§€)
                    if not value_text:
                        link_element = value.select_one('div.value a')
                        if link_element:
                            value_text = link_element.get_text(strip=True)
                    
                    if value_text:
                        # í™•ì¥ëœ í•„ë“œ ë§¤í•‘
                        field_mapping = {
                            'ì‚°ì—…': 'industry',
                            'ê¸°ì—…êµ¬ë¶„': 'size', #ëŒ€ê¸°ì—…/ì¤‘ê²¬/ì¤‘ì†Œ
                            'ì‚¬ì›ìˆ˜': 'description',
                            'ë§¤ì¶œì•¡': 'sales', 
                            'ì„¤ë¦½ì¼': 'founded',
                            'ìë³¸ê¸ˆ': 'capital',
                            'ëŒ€í‘œì': 'ceo',
                            'ì£¼ìš”ì‚¬ì—…': 'business',
                            '4ëŒ€ë³´í—˜': 'insurance',
                            'í™ˆí˜ì´ì§€': 'website',
                            'ì£¼ì†Œ': 'location'
                        }
                        
                        field_key = field_mapping.get(label_text)
                        if field_key and not company.get(field_key):
                            company[field_key] = value_text
                            self.logger.debug(f"âœ… ë§¤í•‘ ì„±ê³µ: {label_text} â†’ {field_key}: {value_text}")
                            
        except Exception as e:
            self.logger.debug(f"í…Œì´ë¸” ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")