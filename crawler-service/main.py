#!/usr/bin/env python3
"""
ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import logging
import os
from datetime import datetime

from crawler.crawler_factory import CrawlerFactory
from crawler.base_crawler import CrawlResult


def setup_logging(log_level: str, site_name: str) -> logging.Logger:
    """ë¡œê¹… ì„¤ì •"""
    # ë¡œê·¸ ë ˆë²¨ ë§¤í•‘
    level_mapping = {
        'DEBUG': logging.DEBUG,
        'INFO': logging.INFO,
        'WARNING': logging.WARNING,
        'ERROR': logging.ERROR,
        'CRITICAL': logging.CRITICAL
    }
    
    log_level_int = level_mapping.get(log_level.upper(), logging.INFO)
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    current_time = datetime.now().strftime('%Y%m%d_%H%M')
    log_dir = f'logs/{current_time}'
    os.makedirs(log_dir, exist_ok=True)
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=log_level_int,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'{log_dir}/crawler_{site_name}.log'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¡œê·¸ ë ˆë²¨: {log_level.upper()}")
    return logger


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬')
    parser.add_argument('--site', required=True, 
                       choices=CrawlerFactory.get_supported_sites(),
                       help='í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ ì„ íƒ')
    parser.add_argument('--full', action='store_true',
                       help='ì „ì²´ í¬ë¡¤ë§ (ê¸°ë³¸ê°’: ì‹ ê·œë§Œ)')
    parser.add_argument('--mongodb-uri', 
                       default=None,
                       help='MongoDB ì—°ê²° URI (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” localhost)')
    parser.add_argument('--log-level',
                       default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                       help='ë¡œê·¸ ë ˆë²¨ (ê¸°ë³¸ê°’: INFO)')
    
    args = parser.parse_args()
    
    try:
        # MongoDB URI í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ì¸ìê°€ ì œê³µëœ ê²½ìš°)
        if args.mongodb_uri:
            os.environ['CRAWLER_SERVICE_MONGODB_URI'] = args.mongodb_uri
            print(f"âœ… MongoDB URI ì„¤ì •: {args.mongodb_uri}")
        
        # ë¡œê·¸ ë ˆë²¨ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        os.environ['LOG_LEVEL'] = args.log_level.upper()
        
        # ë¡œê¹… ì„¤ì •
        logger = setup_logging(args.log_level, args.site)
        
        # ì‹¤í–‰ ì •ë³´ ì¶œë ¥
        logger.info(f"ğŸš€ {args.site} í¬ë¡¤ëŸ¬ ì‹œì‘")
        logger.info(f"ğŸ“Š MongoDB URI: {os.getenv('CRAWLER_SERVICE_MONGODB_URI', 'mongodb://localhost:27017/')}")
        logger.info(f"ğŸ”§ ì‹¤í–‰ ëª¨ë“œ: {'ì „ì²´ í¬ë¡¤ë§' if args.full else 'ì‹ ê·œë§Œ'}")
        logger.info(f"ğŸ“‹ ë¡œê·¸ ë ˆë²¨: {args.log_level.upper()}")
        
        # í¬ë¡¤ëŸ¬ ìƒì„±
        crawler = CrawlerFactory.create_crawler(args.site)
        
        with crawler:
            # í¬ë¡¤ë§ ì‹¤í–‰
            result = crawler.crawl_site(args.site, full_crawl=args.full)
            
            # ê²°ê³¼ ì¶œë ¥
            result.print_summary()
            
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()